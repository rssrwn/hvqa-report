\documentclass[../report.tex]{subfiles}


\begin{document}


\chapter{Hybrid Property, Event and Relation Learner}
\label{chapter:h-perl}

We outline a novel paradigm for solving the VideoQA problem. This structure merges deep learning and logic-based machine learning and inference methods in an attempt to learn the concepts required to answer the questions, hence we name it Hybrid Property, Event and Relation Learner (H-PERL). Section~\ref{section:h-perl-arch} outlines the structure of an H-PERL model, while Section~\ref{section:common-components} discusses the implementation of a number of H-PERL components which are common to all models shown in this report.

\section{Architecture}
\label{section:h-perl-arch}

% Introduce Hybrid VideoQA Model architecture. Talk about how this model models an environment using objects, properties, relations, actions and events. (show generic architecture diagram)

TODO: show diagram

Hybrid VideoQA is a generic, pipelined structure for finding an answer to a set of questions, given a video. The pipeline is composed of a number of components which, when strung together, form a model (a model can be thought of as a specific 'instance' of the Hybrid VideoQA pipeline). A number of models trained on our Sea-World (TODO rename?) dataset are outlined in Chapter~\ref{chapter:implementation}.

A Hybrid VideoQA pipeline assumes that all of the information in an environment which is required to answer the questions can be modeled using: objects; binary relations between objects; and events between two consecutive frames in the video. For many environments, simple environments (like our Sea-World dataset) in particular, this assumption holds. However for many other VideoQA environments, particularly those set in the real-world, this assumption may not be suitable. For example, we may not always be able to extract objects from a video (as in the case of abstract nouns), and yet information on these objects may still be required to answer the questions. The Hybrid VideoQA structure does also not allow for more complex relations between objects.

As shown in Figure TODO, a Hybrid VideoQA model contains the following components:
\begin{itemize}
  \item \textbf{Object Detector}. Produces bounding boxes and classes for each object in each frame of the video. The object detector also needs run an object tracking algorithm to assign each object in the video a unqiue identifier. Ideally the object tracker would correctly identify all objects in the video, however errors in the object detection stage can confuse the tracker as to which identifier should be assigned to each object. In this case some form of error correction may be helpful. TODO

  \item \textbf{Property Extractor}. Given a set of properties, classifies the value of each property for each object in each frame of the video. As mentioned in Chapter~\ref{chapter:dataset}, possible properties could be colour or rotation. The property component does not classify the position or class of an object (this is always handled by the detector).

  \item \textbf{Relation Classifier}.

  \item \textbf{Event Detector}.

  \item \textbf{QA System}.
\end{itemize}

% TODO add this to property extractor discussion
% However, since the \textbf{object} rule in the grammar may contain a property value, knowledge of object properties may be required to work out which object a question is referring to, creating a `chicken-and-egg' problem. For example, if a question asked ``What colour was the upward-facing fish in frame 12?", and there three fish (with unique rotations) in frame 12, one would need knowledge of object properties in order to select a image of a fish to train with. A model would need to be capable of overcoming this problem if it is to make full use of the dataset.

\section{Common Components}
\label{section:common-components}

Give an intro to the each of the components common to all models: object detector, tracker, (QA system), QA parser.



\end{document}
