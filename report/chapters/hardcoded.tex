\documentclass[../report.tex]{subfiles}


\begin{document}


\chapter{Hardcoded Model}
\label{chapter:hardcoded}

Our first H-PERL implementation is the `hardcoded' model. The hardcoded model makes use of a mixture of manually engineered components and components which are trained on the full-data version of the OceanQA dataset. This model should not be taken as a solution to the VideoQA task, since it would be labourious and, in some cases, impossible to rewrite components for each new dataset envrionment. Instead we intend this model to be used as a benchmark for the OceanQA dataset, against which other VideoQA implementations can be evaluated.


\section{Properties}

As mentioned in Chapter~\ref{chapter:h-perl} the role of the properties component is to take an image of an object (as produced by the object detector) and, in the case of the OceanQA dataset, to return the colour and rotation of the object.

Since the object image is a 3D tensor\footnote{Height, width and RGB colour channels make up the three dimensions} of raw pixel values, a convolutional neural network is an excellent candidate for the implementation of the properties component. Other computer vision techniques for machine learning such as decision trees, random forests and SVMs require thousands of manually engineered filters to be applied to the image, whereas convolutional networks can learn a much smaller set of filters based on the training data.




\section{Relations}


\section{Events}


\section{Error Correction}



\end{document}
