\documentclass[../report.tex]{subfiles}


\begin{document}


\chapter{Hardcoded Model}
\label{chapter:hardcoded}

Our first H-PERL implementation is the `hardcoded' model. The hardcoded model makes use of a mixture of manually engineered components and components which are trained on the full-data version of the OceanQA dataset. This model should not be taken as a solution to the general VideoQA problem, since it would be labourious to rewrite components for each new dataset environment. Instead we intend this model to be used as a benchmark for the OceanQA dataset, against which other VideoQA implementations can be evaluated.

Full details on the performance of the model, as well as the performance of some of the individual components is given in Chapter~\ref{chapter:evaluation}.


\section{Properties}

As mentioned in Chapter~\ref{chapter:h-perl}, the role of the properties component is to take an image of an object (as produced by the object detector) and, in the case of the OceanQA dataset, to return the colour and rotation of the object.

Since the object image is a 3D tensor\footnote{Height, width and RGB colour channels make up the three dimensions} of raw pixel values, a convolutional neural network is an excellent candidate for the implementation of the properties component. Other computer vision techniques for machine learning such as decision trees, random forests and SVMs require thousands of manually engineered filters to be applied to the image, whereas convolutional networks can learn a much smaller set of filters based on the training data.

Figure~\ref{fig:prop-network} shows the architecture of the properties network. The first part of this network encodes the object image into a 1024 dimenional latent vector using a series of convolutional and fully-connected layers. A set of fully-connected layers, one for each property, each take the vector encoding of the object and produce a vector of  real numbers. The size of each vector is equal to the number of possible values that property can take.

The final output of the network is a set of probability distributions, one for each property, over the set of possible values that property can take. As is standard in a multiclass classification problem, the softmax function is applied to each set of property values in order to construct the probability distribution. The softmax function guarantees that the elements of a vector sum to one and that each element is greater than or equal to zero, hence softmax creates a discrete probability distribution. The softmax function for a vector $\mathbf{z} \in \mathbb{R}^K$ is as follows:
\begin{equation}
  \sigma(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}} \text{ for } i = 1,...,K
\end{equation}

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{prop-network}
  \caption{Architecture of the properties component neural network. An object is encoded into a latent vector, before a set of fully-connected layers produce a set of probability distributions over the property values. Batch normalisation is applied between convolutional layers. FM stands for feature maps.}
  \label{fig:prop-network}
\end{figure}

The full-data version of the OceanQA dataset labels both the colour and rotation of every object in every frame. This makes training a neural network relatively straightforward; we collate all of the objects and their properties in the dataset, resize each object to 32x32 pixels, convert each property into a one-hot encoded vector and train the network using batches of 256 objects with a learning rate of 0.001 for 2. The cross-entropy loss is calculated for each property and these are summed to give an overall loss. The cross-entropy loss between a predicted probability distribution vector $\mathbf{p} \in \mathbb{R}^K$ and a one-hot encoded classification vector $\mathbf{y} \in \mathbb{R}^K$, where $K$ is the number of classes, is as follows:
\begin{equation}
  H(\mathbf{p}, \mathbf{y}) = - \sum_{c=1}^{K} y_c \text{ log}(p_c)
\end{equation}

When the H-PERL model is being evaluated each object in the video is applied to the network (batched together for efficiency) and a set of probability distributions is produced. For each object, the property value for a particular property is given by the most probable element of the vector.


\section{Relations}

The job of the relations component is to list all instances of relevant binary relations between objects in each frame of the video. Since there is only one relevant binary relation in the OceanQA dataset, the job of this component is simply to list the instances of the \textit{close} relation.

The relations component of the hardcoded model contains a hand-written binary classification algorithm for determining whether two objects are close or not. For a given video, this algorithm is applied to every pair of objects in every frame of the video in order to list all instances of the relation. The object arguments of the closeness algorithm are given in symbolic form, rather than as raw pixel matrices. This means the algorithm is heavily reliant on accurate information from the object detector - the position tuple in particular. Although the closeness algorithm doesn't make use of the property component's extracted features, other algorithms for determining binary relations may require these.

The algorithm for determining the closeness of two objects is, in fact, identical to the algorithm used when constructing the dataset. The algorithm uses the idea of an expanded box around each object; the objects are close if their boxes overlap, as can be seen in Figure~\ref{fig:change-colour}. The algorithm was discussed alongside techniques used to create the dataset in Chapter~\ref{chapter:dataset} and is shown in Algorithm~\ref{algo:close_to}.

Clearly, since the algorithm used to construct the dataset and the algorithm used to find the relations between objects are exactly the same, the relations component achieves perfect accuracy provided the object detection is exactly correct. Although this may seem like cheating, since in general it is not possible to know the underlying rules of the dataset, we reiterate that the hardcoded model is not a solution to VideoQA tasks in general, but rather is specific to the OceanQA dataset, and can be used for comparisons with other models.

\begin{figure}[h]
  \begin{subfigure}{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{change-colour-0.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{change-colour-1.png}
  \end{subfigure}
  \caption{Diagrams showing the octopus before and after moving close to a purple rock, and therefore turning purple itself. The brown dashed lines show the bounding boxes expanded by 5 pixels on each side around the objects. If these boxes overlap the objects are deemed to be close to one another.}
  \label{fig:change-colour}
\end{figure}

Another consideration for this approach to relation classification is speed; each relation classification algorithm (of which there is only one in the OceanQA dataset, but, in general there may be many) looks at every possible pair of objects in every frame of the video. Assuming that each relation classifier operates in constant time, this creates an overall algorithmic complexity of $\mathcal{O}(kmn^2)$, where $k$ is the number of frames per video, $m$ is the number of relations to be classified and $n$ is the number of objects in each frame. Despite this we found that the time taken by the relation component was small relative to other components in the H-PERL model. Chapter~\ref{chapter:evaluation} outlined the full details of the component's performance, including details on the time taken during evaluation.


\section{Events}

As mentioned in Chapter~\ref{chaper:h-perl}, the role of the event detection component is to list all of the actions and effects which take place between each pair of consecutive frames of a given video. Since preceeding components have extracted a number of object and frame-level symbolic features already, the event detector in the hardcoded model works only with these features, as opposed to viewing the raw pixels in each frame.

For the hardcoded model, the event detector is implemented by, firstly, searching through all possible combinations of actions. The component assumes that there is only one action per frame and that only non-static objects can be the cause of an action. Each combination of actions is then applied to a hand-written $\mathcal{AL}$ model of the environment to generate the set of symbolic features which would be expected to be observed should the set of actions have occurred. This set of features is then compared to the set of features which were actually observed - the set of features extracted by preceeding components. Finally, the combination of actions which lead to the best fit with the observed data are selected.

For each set of possible actions and their associated features generated by the $\mathcal{AL}$ model, a set of manually engineered ASP rules is used to find the corresponding set of effects. For example, if the octopus moves\footnote{Moving during frame $i$ refers to an object moving between frame $i$ and frame $i+1$.} during frame $i$ and is close to a blue rock during frame $i+1$, then we can infer that the \textit{change colour} effect occurs during frame $i$.


\subsection{The $\mathcal{AL}$ Model}

As described in Chapter~\ref{chapter:background}, $\mathcal{AL}$ is a formal model for describing the behaviour of a dynamic system. An $\mathcal{AL}$ system consists of a set of states and some way of transitioning between states using actions. We model our video as an $\mathcal{AL}$ system by considering each frame as a state and object actions between frames as $\mathcal{AL}$ actions. In $\mathcal{AL}$ a state is described by a set of fluents. Each fluent, $\textit{$<$f$>$}$, can be wrapped up in a predicate, $holds(\textit{$<$f$>$}, \textit{$<$i$>$})$, where \textit{$<$i$>$} is a timestep in the video, which means that fluent \textit{$<$f$>$} is true at step \textit{$<$i$>$}. Conversely, $-holds(\textit{$<$f$>$}, \textit{$<$i$>$})$ means that \textit{$<$f$>$} is not true at step \textit{$<$i$>$}. An OceanQA-specific example of a set of $\mathcal{AL}$ states and transitions between them is shown in Figure~\ref{fig:al-states}.

\begin{figure}
  \centering
  \includegraphics[width=0.9\textwidth]{al-states}
  \caption{A simplified example set of $\mathcal{AL}$ states and transitions. States which model the OceanQA environment contain many more fluents than are shown here.}
  \label{fig:al-states}
\end{figure}

As mentioned above, the optimal set of actions is found by comparing the observed features of the environment with an internal $\mathcal{AL}$ model of the environment. In the rest of this section we outline the $\mathcal{AL}$ system description for the OceanQA environment. This system description can then be written in ASP using the encoding provided in Appendix~\ref{appendix-al}.

Firstly, we outline the types involved in the $\mathcal{AL}$ system description, which are as follows:
\begin{itemize}
  \item Properties, including class and position, are modelled as \textit{inertial fluents} - their values can change as a direct result of the actions taken. Notice, however, that the number of possible position values is very large - $256^4$, hence, rather than allowing all possible values, we only include values that have been observed in the video, all other values are not modelled. This applied to all properties, class and position fluents.

  \item An additional inertial fluent, $exists(\textit{$<$id$>$})$, is also required. Intuitively, it means that an object with identifier \textit{$<$id$>$} is present in the current timestep.

  \item The close relation, which is written in ASP as $close(Id1, Id2)$ when an object with identifier $Id1$ is close to an object with identifier $Id2$, is considered a \textit{defined fluent}. Although they are modelled as defined fluents, their thruthiness cannot altered by the events component, since relation classification is handled by the relations component. Therefore, these fluents are copied directly across from the observed data into the $\mathcal{AL}$ model.

  \item Actions are, of course, modelled by the \textit{action} type. The action itself takes a single argument - the object's identifier. In ASP each action is written as $\textit{$<$action$>$}(\textit{$<$id$>$})$.
\end{itemize}

The domain independent rules for the OceanQA environment are as outlined in Appendix~\ref{appendix-al}, with the following addition:
\begin{equation}
  -holds(F, 0) \text{ :- } fluent(inertial, F), not \text{ } holds(F, 0).
\end{equation}

This rule ensures that the initial state of the system is complete for inertial fluents - all inertial fluents are either true or false. While this isn't a requirement for $\mathcal{AL}$ systems, it simplifies the rest of the model since there is no uncertainty about the system's state.

Finally, we outline the domain dependent $\mathcal{AL}$ statements for the actions in the OceanQA environment:
\begin{itemize}
  \item The \textit{move} action causes the object to move 15 pixels in the direction of rotation. This means that, in frame $i+1$, the object is no longer in the position it was in during frame $i$. We use a Python function, $new\_pos$, in the ASP program to calculate the object's next position. This function is called using the $@$ symbol. The $\mathcal{AL}$ statements for the move action are the following:
  \begin{gather*}
    move(Id) \textbf{ causes } position(@new\_pos(P, R), Id) \textbf{ if } position(P, Id), rotation(R, Id) \\
    move(Id) \textbf{ causes } -position(position(P, Id)) \textbf{ if } position(P, Id)
  \end{gather*}

  \item $\mathcal{AL}$ statements follow the same pattern for both rotation actions. The Python function, $new\_rot$, is used to calculate the new rotation for the object, it takes the previous rotation and the type of rotation as arguments. In the following $\mathcal{AL}$ statements \textit{$<$d$>$} is used to refer to the direction of rotation:
  \begin{gather*}
    rotate\_\textit{$<$d$>$}(Id) \textbf{ causes } rotation(@new\_rot(R, rotate\_\textit{$<$d$>$}) \textbf{ if } rotation(R, Id) \\
    rotate\_\textit{$<$d$>$}(Id) \textbf{ causes } -rotation(R, Id) \textbf{ if } rotation(R, Id)
  \end{gather*}
\end{itemize}

Effects are defined using ASP rules with a combination of actions, observed features and $\mathcal{AL}$ model features in the body. These rules are not strictly part of the $\mathcal{AL}$ model, however, as shown below, some of the $\mathcal{AL}$ fluents are updated using effects.

% We also need to ensure the octopus' colour is updated when it comes close to rock. In the following $\mathcal{AL}$ statements we use a helper predicate
% \begin{gather*}
%   move(Id) \textbf{ causes } colour(New, Id) \textbf{ if } colour(Old, Id), change_colour(Old, New, Id) \\
%   move(Id) \textbf{ causes } -colour(Old, Id) \textbf{ if } colour(Old, Id), change_colour(Old, New, Id)
% \end{gather*}


\subsection{Action Optimisation}


\section{Error Correction}



\end{document}
