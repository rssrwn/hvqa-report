\documentclass[../report.tex]{subfiles}


\begin{document}


\chapter{Hardcoded Model}
\label{chapter:hardcoded}

Our first H-PERL implementation is the `hardcoded' model. The hardcoded model makes use of a mixture of manually engineered components and components which are trained on the full-data version of the OceanQA dataset. This model should not be taken as a solution to the general VideoQA problem, since it would be labourious to rewrite components for each new dataset environment. Instead we intend this model to be used as a benchmark for the OceanQA dataset, against which other VideoQA implementations can be evaluated.

Full details on the performance of the model, as well as the performance of some of the individual components is given in Chapter~\ref{chapter:evaluation}.


\section{Properties}

As mentioned in Chapter~\ref{chapter:h-perl}, the role of the properties component is to take an image of an object (as produced by the object detector) and, in the case of the OceanQA dataset, to return the colour and rotation of the object.

Since the object image is a 3D tensor\footnote{Height, width and RGB colour channels make up the three dimensions} of raw pixel values, a convolutional neural network is an excellent candidate for the implementation of the properties component. Other computer vision techniques for machine learning such as decision trees, random forests and SVMs require thousands of manually engineered filters to be applied to the image, whereas convolutional networks can learn a much smaller set of filters based on the training data.

Figure~\ref{fig:prop-network} shows the architecture of the properties network. The first part of this network encodes the object image into a 1024 dimenional latent vector using a series of convolutional and fully-connected layers. A set of fully-connected layers, one for each property, each take the vector encoding of the object and produce a vector of  real numbers. The size of each vector is equal to the number of possible values that property can take.

The final output of the network is a set of probability distributions, one for each property, over the set of possible values that property can take. As is standard in a multiclass classification problem, the softmax function is applied to each set of property values in order to construct the probability distribution. The softmax function guarantees that the elements of a vector sum to one and that each element is greater than or equal to zero, hence softmax creates a discrete probability distribution. The softmax function for a vector $\mathbf{z} \in \mathbb{R}^K$ is as follows:
\begin{equation}
  \sigma(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}} \text{ for } i = 1,...,K
\end{equation}

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{prop-network}
  \caption{Architecture of the properties component neural network. An object is encoded into a latent vector, before a set of fully-connected layers produce a set of probability distributions over the property values. Batch normalisation is applied between convolutional layers. FM stands for feature maps.}
  \label{fig:prop-network}
\end{figure}

The full-data version of the OceanQA dataset labels both the colour and rotation of every object in every frame. This makes training a neural network relatively straightforward; we collate all of the objects and their properties in the dataset, resize each object to 32x32 pixels, convert each property into a one-hot encoded vector and train the network using batches of 256 objects with a learning rate of 0.001 for 2. The cross-entropy loss is calculated for each property and these are summed to give an overall loss. The cross-entropy loss between a predicted probability distribution vector $\mathbf{p} \in \mathbb{R}^K$ and a one-hot encoded classification vector $\mathbf{y} \in \mathbb{R}^K$, where $K$ is the number of classes, is as follows:
\begin{equation}
  H(\mathbf{p}, \mathbf{y}) = - \sum_{c=1}^{K} y_c \text{ log}(p_c)
\end{equation}

When the H-PERL model is being evaluated each object in the video is applied to the network (batched together for efficiency) and a set of probability distributions is produced. For each object, the property value for a particular property is given by the most probable element of the vector.


\section{Relations}

The job of the relations component is to list all instances of relevant binary relations between objects in each frame of the video. Since there is only one relevant binary relation in the OceanQA dataset, the job of this component is simply to list the instances of the \textit{close} relation.

The relations component of the hardcoded model contains a hand-written binary classification algorithm for determining whether two objects are close or not. For a given video, this algorithm is applied to every pair of objects in every frame of the video in order to list all instances of the relation. The object arguments of the closeness algorithm are given in symbolic form, rather than as raw pixel matrices. This means the algorithm is heavily reliant on accurate information from the object detector - the position tuple in particular. Although the closeness algorithm doesn't make use of the property component's extracted features, other algorithms for determining binary relations may require these.

The algorithm for determining the closeness of two objects is, in fact, identical to the algorithm used when constructing the dataset. The algorithm uses the idea of an expanded box around each object; the objects are close if their boxes overlap, as can be seen in Figure~\ref{fig:change-colour}. The algorithm was discussed alongside techniques used to create the dataset in Chapter~\ref{chapter:dataset} and is shown in Algorithm~\ref{algo:close_to}.

Clearly, since the algorithm used to construct the dataset and the algorithm used to find the relations between objects are exactly the same, the relations component achieves perfect accuracy provided the object detection is exactly correct. Although this may seem like cheating, since in general it is not possible to know the underlying rules of the dataset, we reiterate that the hardcoded model is not a solution to VideoQA tasks in general, but rather is specific to the OceanQA dataset, and can be used for comparisons with other models.

\begin{figure}[h]
  \begin{subfigure}{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{change-colour-0.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{change-colour-1.png}
  \end{subfigure}
  \caption{Diagrams showing the octopus before and after moving close to a purple rock, and therefore turning purple itself. The brown dashed lines show the bounding boxes expanded by 5 pixels on each side around the objects. If these boxes overlap the objects are deemed to be close to one another.}
  \label{fig:change-colour}
\end{figure}

Another consideration for this approach to relation classification is speed; each relation classification algorithm (of which there is only one in the OceanQA dataset, but, in general there may be many) looks at every possible pair of objects in every frame of the video. Assuming that each relation classifier operates in constant time, this creates an overall algorithmic complexity of $\mathcal{O}(kmn^2)$, where $k$ is the number of frames per video, $m$ is the number of relations to be classified and $n$ is the number of objects in each frame. Despite this we found that the time taken by the relation component was small relative to other components in the H-PERL model. Chapter~\ref{chapter:evaluation} outlined the full details of the component's performance, including details on the time taken during evaluation.


\section{Events}

As mentioned in Chapter~\ref{chaper:h-perl}, the role of the event detection component is to list all of the actions and effects which take place between each pair of consecutive frames of a given video. Since preceeding components have extracted a number of object and frame-level symbolic features already, the event detector in the hardcoded model works only with these features, as opposed to viewing the raw pixels in each frame.

For the hardcoded model, the event detector is implemented by, firstly, searching through all possible combinations of actions. The component assumes that there is only one action per frame and that only non-static objects can be the cause of an action. Each combination of actions is then applied to a hand-written $\mathcal{AL}$ model of the environment to generate the set of symbolic features which would be expected to be observed should the set of actions have occurred. This set of features is then compared to the set of features which were actually observed - the set of features extracted by preceeding components. Finally, the combination of actions which lead to the best fit with the observed data are selected.

For each set of possible actions and their associated features generated by the $\mathcal{AL}$ model, a set of manually engineered ASP rules is used to find the corresponding set of effects. For example, if the octopus moves\footnote{Moving during frame $i$ refers to an object moving between frame $i$ and frame $i+1$.} during frame $i$ and is close to a blue rock during frame $i+1$, then we can infer that the \textit{change colour} effect occurs during frame $i$.


\subsection{The $\mathcal{AL}$ Model}

As described in Chapter~\ref{chapter:background}, $\mathcal{AL}$ is a formal model for describing the behaviour of a dynamic system. An $\mathcal{AL}$ system consists of a set of states and some way of transitioning between states using actions. We model our video as an $\mathcal{AL}$ system by considering each frame as a state and object actions between frames as $\mathcal{AL}$ actions. In $\mathcal{AL}$ a state is described by a set of fluents. Each fluent, $\textit{$<$f$>$}$, can be wrapped up in a predicate, $holds(\textit{$<$f$>$}, \textit{$<$i$>$})$, where \textit{$<$i$>$} is a timestep in the video, which means that fluent \textit{$<$f$>$} is true at step \textit{$<$i$>$}. Conversely, $\neg holds(\textit{$<$f$>$}, \textit{$<$i$>$})$ means that \textit{$<$f$>$} is not true at step \textit{$<$i$>$}. An OceanQA-specific example of a set of $\mathcal{AL}$ states and transitions between them is shown in Figure~\ref{fig:al-states}.

\begin{figure}
  \centering
  \includegraphics[width=0.9\textwidth]{al-states}
  \caption{A simplified example set of $\mathcal{AL}$ states and transitions. States which model the OceanQA environment contain many more fluents than are shown here.}
  \label{fig:al-states}
\end{figure}

As mentioned above, the optimal set of actions is found by comparing the observed features of the environment with an internal $\mathcal{AL}$ model of the environment. In the rest of this section we outline the $\mathcal{AL}$ system description for the OceanQA environment. This system description can then be written in ASP using the encoding provided in Appendix~\ref{appendix-al}.

Firstly, we outline the types involved in the $\mathcal{AL}$ system description, which are as follows:
\begin{itemize}
  \item Properties, including class and position, are modelled as \textit{inertial fluents} - their values can change as a direct result of the actions taken. Notice, however, that the number of possible position values is very large - $256^4$, hence, rather than allowing all possible values, we only include values that have been observed in the video, all other values are not modelled. This applied to all properties, class and position fluents.

  \item An additional inertial fluent, $exists(\textit{$<$id$>$})$, is also required. Intuitively, it means that an object with identifier \textit{$<$id$>$} is present in the current timestep.

  \item The close relation, which is written in ASP as $close(Id1, Id2)$ when an object with identifier $Id1$ is close to an object with identifier $Id2$, is considered a \textit{defined fluent}. Although they are modelled as defined fluents, their thruthiness cannot altered by the events component, since relation classification is handled by the relations component. Therefore, these fluents are copied directly across from the observed data into the $\mathcal{AL}$ model.

  \item We add a further defined fluent, which also comes directly from the observed information. This fluent is called $disappear(\textit{$<$id$>$})$, and, naturally, means that an object with identifier \textit{$<$id$>$} disappears immediately after the current timestep.

  \item Actions are, of course, modelled by the \textit{action} type. The action itself takes a single argument - the object's identifier. In ASP each action is written as $\textit{$<$action$>$}(\textit{$<$id$>$})$.
\end{itemize}

The domain independent rules for the OceanQA environment are as outlined in Appendix~\ref{appendix-al}, with the following addition:
\begin{equation}
  \neg holds(F, 0) \text{ :- } fluent(inertial, F), not \text{ } holds(F, 0).
\end{equation}

This rule ensures that the initial state of the system is complete for inertial fluents - all inertial fluents are either true or false. While this isn't a requirement for $\mathcal{AL}$ systems, it simplifies the rest of the model since there is no uncertainty about the system's state.

Finally, we outline the domain dependent $\mathcal{AL}$ statements for the OceanQA environment:
\begin{itemize}
  \item The \textit{move} action causes the object to move 15 pixels in the direction of rotation. This means that, in frame $i+1$, the object is no longer in the position it was in during frame $i$. We use a Python function, $new\_pos$, in the ASP program to calculate the object's next position. This function is called using the $@$ symbol. The $\mathcal{AL}$ \textit{causal laws} for the move action are the following:
  \begin{gather*}
    move(Id) \textbf{ causes } position(@new\_pos(P, R), Id) \textbf{ if } position(P, Id), rotation(R, Id) \\
    move(Id) \textbf{ causes } \neg position(position(P, Id)) \textbf{ if } position(P, Id)
  \end{gather*}

  \item $\mathcal{AL}$ causal laws follow the same pattern for both rotation actions. The Python function, $new\_rot$, is used to calculate the new rotation for the object, it takes the previous rotation and the type of rotation as arguments. In the following $\mathcal{AL}$ causal laws \textit{$<$d$>$} is used to refer to the direction of rotation:
  \begin{gather*}
    rotate\_\textit{$<$d$>$}(Id) \textbf{ causes } rotation(@new\_rot(R, rotate\_\textit{$<$d$>$}) \textbf{ if } rotation(R, Id) \\
    rotate\_\textit{$<$d$>$}(Id) \textbf{ causes } \neg rotation(R, Id) \textbf{ if } rotation(R, Id)
  \end{gather*}

  \item In order to ensure that objects are modelled as non-existent after they have disappeared, the following causal law is added:
  \begin{equation*}
    move(Id) \textbf{ causes } \neg exists(Id) \textbf{ if } exists(Id), disappear(Id)
  \end{equation*}

  \item Finally, the following \textit{executability conditions} are added to reduce the size of the search space for the optimiser:
  \begin{gather*}
    \textbf{impossible } move(Id) \textbf{ if } \neg exists(Id) \\
    \textbf{impossible } rotate\_clockwise(Id) \textbf{ if } \neg exists(Id) \\
    \textbf{impossible } rotate\_anticlockwise(Id) \textbf{ if } \neg exists(Id)
  \end{gather*}
\end{itemize}

The use of the $exists(\textit{$<$id$>$})$ fluent is an intuitive way of encoding the knowledge that when an object disappears it no longer exists, and, if an object does not exist, it cannot participate in any actions. Without this predicate, encoding knowledge like this would be tricky because the $disappear(\textit{$<$id$>$})$ fluent appears (at most) once during the video.

In the remainder of this section we diverge from $\mathcal{AL}$ orthodoxy in order to model effects. Effects are defined by ASP rules with a combination of actions and $\mathcal{AL}$ fluents in the body. These rules can mostly be considered to observe the $\mathcal{AL}$ model, rather than affect it. However, an exception is made for the rules which update the octopus' colour.

\begin{figure}
  \centering
  \begin{subfigure}{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{eat-fish-0.png}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{eat-fish-1.png}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{eat-fish-2.png}
  \end{subfigure}
  \caption{An example of the \textit{eat a fish} effect.}
  \label{fig:eat-fish}
\end{figure}

Firstly, for the \textit{eat a fish} (an example of which is shown in Figure~\ref{fig:eat-fish}) and \textit{eat a bag} effects, the following ASP rules are used:
\begin{equation}
  \begin{split}
    occurs\_effect(eat\_a\_fish(Octo), I) \text{ :- } & occurs\_action(move(Octo), I), \\
    & holds(class(fish, Fish), I), \\
    & holds(disappear(Fish), I).
  \end{split}
\end{equation}

\begin{equation}
  \begin{split}
    occurs\_effect(eat\_a\_bag(Octo), I) \text{ :- } & occurs\_action(move(Octo), I), \\
    & holds(class(bag, Bag), I), \\
    & holds(disappear(Bag), I), \\
    & holds(disappear(Octo), I).
  \end{split}
\end{equation}

We also need to ensure the octopus' colour is updated when it comes close to rock. To do this we use the following helper predicate:
\begin{equation}
  \begin{split}
    change\_colour(Old, New, Id, I-1) \text{ :- } & holds(class(octopus, Id), I), \\
    & holds(close(Id, IdRock), I), \\
    & holds(colour(Old, Id), I-1), \\
    & holds(class(rock, IdRock), I), \\
    & holds(colour(New, IdRock), I), \\
    & Old \text{ } != New, step(I-1).
  \end{split}
\end{equation}

Finally, the octopus' colour is updated using the follwing rules:
\begin{gather}
  holds(colour(New, Id), I+1) \text{ :- } change\_colour(Old, New, Id, I). \label{eqn:update-colour} \\
  \neg holds(colour(Old, Id), I+1) \text{ :- } change\_colour(Old, New, Id, I). \label{eqn:neg-update-colour}
\end{gather}

Rules~\ref{eqn:update-colour} and~\ref{eqn:neg-update-colour} alter the $\mathcal{AL}$ state, despite not being generated from $\mathcal{AL}$ statements. There are other $\mathcal{AL}$ models for the OceanQA environment (not discussed here) which do not suffer from the same problem. However, these models can be more complex, which leads to higher probability of human error. Instead we prefer to make a slight adaption to the $\mathcal{AL}$ rules in order to allow a simpler model to be defined.


\subsection{Action Optimisation}

As we have seen, the $\mathcal{AL}$ model uses the $holds(\textit{$<$f$>$}, \textit{$<$i$>$})$ predicate to store its information, but the observed data uses $obs(\textit{$<$f$>$}, \textit{$<$i$>$})$. This distinction is by design; it separates the information (particularly that which is modelled by inertial fluents) in the two data models. However, we need some way of comparing the two models in order to optimise the action combination.

Firstly, in order to ensure that both data models start with the same information, the fluents for the initial frame in the observed data are copied over to the $\mathcal{AL}$ model. Secondly, the ASP optimisation program generates all possible action combinations and applies each combination to the $\mathcal{AL}$ model to generate the data expected from that particular set of actions. These sets of action can be generated in ASP using the following choice rule:
\begin{equation}
  occurs\_action(A, I) : action(A) \text{ :- } step(I+1), I >= 0.
\end{equation}

Finally, in order to give each set of actions a score, the expected data is compared with the observed data and the number of mismatched fluents is counted. In ASP a \textit{weak constraint} can be used to assign a cost to an answer set and the ASP optimiser searches for the answer set with the lowest cost. We use the following two weak constraints to optimise the action search:
\begin{gather}
  \text{:$\sim$ } \neg obs(exists(Id), I), holds(exists(Id), I). [1@1, exists(Id), I] \label{eqn:exists-opt} \\
  \text{:$\sim$ } obs(F, I), \neg holds(F, I). [1@2, F, I] \label{eqn:fluent-opt}
\end{gather}

Rule~\ref{eqn:exists-opt} is given a higher priority, so ASP searches for a set of answer sets which minimise the number of times the body is true before considering Rule~\ref{eqn:fluent-opt}. Rule~\ref{eqn:exists-opt} says that we prefer answer sets with the fewest mismatches due to the $exists\textit{$<$id$>$}$ fluent. Rule~\ref{eqn:fluent-opt}, on the other hand, says that we prefer answer sets with the fewest mismatches between all fluents. Rule~\ref{eqn:exists-opt} is given a higher priority, firstly, because it is more specific, and, secondly, because we consider an error in modelling the existence of an object as more significant than errors in modelling other fluents.

The following two hard constraints are also applied to ensure that basic rules of the OceanQA environment are not broken:
\begin{gather}
  \text{:- } obs(class(octopus, Id), I), occurs\_action(nothing(Id), I). \label{eqn:occurs-nothing-opt1} \\
  \text{:- } \neg obs(class(octopus, Id), I), \neg occurs\_action(nothing(Id), I). \label{eqn:occurs-nothing-opt2}
\end{gather}

The use of a hard rather than weak constraints in Rules~\ref{eqn:occurs-nothing-opt1} and~\ref{eqn:occurs-nothing-opt2} improves the speed of the ASP optimisation. However, the weak constraints in Rules~\ref{eqn:exists-opt} and~\ref{eqn:fluent-opt} are necessary since the program must be able to deal with noisy data. If hard constraints were used instead any noise in the data could rule out all possible answer sets.

All of the above rules, as well as the ASP encoding of the $\mathcal{AL}$ model and the observed data, are run in a single ASP program which attempts to find the optimal set of actions for the video. While it may not always be able to find the optimal, it tries to minimise the difference between the observed data and the expected data.

% TODO talk about speed and ASPs use of branch-and-bound


\section{Error Correction}



\end{document}
