\documentclass[../report.tex]{subfiles}


\begin{document}


\chapter{Trained Model}
\label{chapter:trained}


In contrast to the hardcoded model, the trained H-PERL model does not use manually engineered relations or events components, and must therefore rely on using components which can be trained. The trained model also uses the QA-data version of the OceanQA dataset, rather than the full-data version which the hardcoded model was able to use to train its properties component. This means that the trained model needs to rely on the data contained in QA pairs alone to train its components. The three trained core components of the model are outlined in this chapter. After training each component, the component is used to extract relevant symbolic information from the training data before the next component in the architecture is trained.

As with the hardcoded model, full performance evaluation details of the trained model and some of its components can be found in Chapter~\ref{chapter:evaluation}.


\section{Properties}

As mentioned in Chapter~\ref{chapter:dataset}, property questions in the OceanQA dataset ask the model to find a property value for a specific object. This object, however, can contain a reference to a property value. This means that, in some cases, knowledge of object properties is required in order to find the specified object in the frame. For example, if a question asked ``What colour was the upward-facing fish in frame 12?", and there three fish, each with unique rotations, in frame 12, one would need knowledge of object properties in order to select the correct image of the fish. This creates a `chicken-and-egg' problem when collecting the training data; the model needs a trained property extractor in order to find the images to train the property extractor with. This means the training data cannot be collated in the same way as the hardcoded model.

In this section we propose a solution to overcome this problem which utilises semi-supervised learning in order to label all of the objects in the dataset with property values. Once these labels have been found, the property component can be trained in the same way as the hardcoded model, outlined in Chapter~\ref{chapter:hardcoded}.

The first step of the algorithm for finding these labels involves training an autoencoder neural network to extract a 16-dimensional latent vector from each object image. This network is trained in an unsupervised manner using a sample of 40,000 of the objects detected by the object detector in the training data, where each object type is equally represented in the sample. The architecture of the network is shown in Figure~\ref{fig:ae-arch}. Each object image is resized to 16$\mathsf{x}$16 pixels and the network is trained with a learning rate of $0.001$ for 5 epochs with a mean-absolute error (MAE) loss function.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{ae-arch}
  \caption{An illustration of the autoencoder architecture. Although not shown here, the network contains batch normalisation layers between each pair of convolution layers. FM stands for feature maps, and the dimensions of each feature maps are given as $(\textit{height}, \textit{width}, \textit{number of feature maps})$.}
  \label{fig:ae-arch}
\end{figure}

The autoencoder allows the component to work with the latent vectors of objects rather than raw object images. At this stage, every object in the training data is encoded using the autoencoder and stored in vector form. After the autoencoder has been trained and all the objects have been encoded, the properties component splits objects in groups based on their class. The component then proceeds to individually apply the following three high-level steps, which are described in further detail in the sections below, to each object type, $t_i$:
\begin{enumerate}
  \item The objects with type $t_i$ are clustered using their latent encoding and each cluster is assigned an integer identifier, $c_j$. More detail on the clustering is outlined in Section~\ref{subsection:trained-clustering}.
  \item The QA pairs will label a number of the objects in the training data with some or all of their property values. ASP can be used to find a mapping from each $c_j$ to a set of property-value pairs. The ASP program used to conduct this mapping is described in Section~\ref{subsection:trained-mapping-optimisaton}.
  \item Once this mapping is known, all of the objects of type $t_i$ in the training data can be labelled. The algorithm for finding these labels is outlined in Section~\ref{subsection:trained-data-labelling}.
\end{enumerate}

As mentioned above, when the property value labels have been found for all objects of all types, the property component can be trained using the same method as the hardcoded model, discussed in Chapter~\ref{chapter:hardcoded}.


\subsection{Clustering}
\label{subsection:trained-clustering}

An analysis of the full-data version of the OceanQA dataset (where objects come fully labelled with their properties) shows that the information encoded in the latent vector of each object separates objects into distinct groups based on their properties. This makes clustering a strong candidate for explicitly separating the objects into distinct groups. The Principle Component Analysis (PCA) projection of the latent vectors for octopus and fish into 2-dimensions is shown in Figure~\ref{fig:latents}.

Clustering of the object latent vectors is done using the \textit{Agglomerative Clustering} algorithm implemented by the \textit{SciKitLearn}\footnote{Available at https://scikit-learn.org/stable/modules/clustering.html} library. While many other clustering algorithms are available, Agglomerative Clustering was found to work efficiently with a large number of samples. Unlike some other clustering algorithms, however, Agglomerative Clustering requires that the number of clusters to be produced is known beforehand.

\begin{figure}[t]
  \begin{subfigure}{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{octopus-latents-rotation.png}
    \caption{Latent space of octopus rotations.}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{fish-latents-rotation.png}
    \caption{Latent space of fish rotations.}
  \end{subfigure}
  \caption{2-dimensional representation of the latent space of octopus and fish images. The colours show different rotation values. Note, these property values come from the full-data version of the OceanQA dataset, not from clustering. Although not shown here, rocks and bags follow a very similar pattern to fish.}
  \label{fig:latents}
\end{figure}

\begin{table}[h!]
  \centering
  \begin{tabular}{ |c|c c c| }
    \hline
    \textbf{Object Type} & \textbf{\#Colours} & \textbf{\#Rotations} & \textbf{\#Clusters} \\
    \hline
    Octopus & 5 & 4 & 20 \\
    Fish    & 1 & 4 & 4 \\
    Bag     & 1 & 4 & 4 \\
    Rock    & 4 & 1 & 4 \\
    \hline
  \end{tabular}
  \caption{Estimates of the number of clusters required for each class, calculated by multiplying the number of colour and rotation values found in the property questions.}
  \label{table:property-combinations}
\end{table}

In order to calculate the number of clusters required for each class, we analyse the number of property values that are linked with that class in the QA pairs. For example, fish only take a single colour, silver, and so only this colour will be mentioned in the property questions that ask about fish. Additionally, fish can take four rotations, and so we expect that all four of these rotations will be mentioned in the questions. In total this gives us four possible property value combinations for fish. We repeat this process for all four object types and list the results in Table~\ref{table:property-combinations}.

In theory, it is possible that this method of calculating the number of clusters required may underestimate, since not all property values may be mentioned in the QA pairs. In practice, however, this makes little difference because if a particular property value is not represented in the QA pairs it is likely to be very rare in the videos and in the evaluation questions.

\begin{figure}
  \centering
  \includegraphics[width=0.75\textwidth]{octopus-clusters.png}
  \caption{Output of the Agglomerative Clustering algorithm for the octopus images.}
  \label{fig:octopus-clusters}
\end{figure}

Figure~\ref{octopus-clusters} shows the result of the clustering algorithm applied to the octopus images, projected into two dimensions. After clustering has been completed for a given class, we assign each cluster (each different colour shown in Figure~\ref{fig:octopus-clusters}) an integer identifier. The data collection problem outlined at the start of this chapter has now been reduced to finding the mapping from each cluster identifier to a set of property-value pairs, which optimises some objective function, for each object type $t_i$. For example, for a cluster $c_j$, we want to find the optimal values for colour and rotation which correspond to $c_j$. We denote the optimal mapping from the set of cluster identifiers $C$ to the set of sets of property-value pairs $P$, for object type $t_i$ as $f_{t_i}^*: C \rightarrow P$.


\subsection{Property Value Optimisation}
\label{subsection:trained-mapping-optimisaton}

In order to find $f_{t_i}^*$, we must first define the objective function. Since we have a set of property questions and answers, we choose to maximise the number of questions answered correctly. It is now necessary to find the questions which correspond to object type $t_i$. We also use the question and answer parsing component (outlined in Chapter~\ref{chapter:h-perl}) to extract the relevant symbolic information from the question and from the answer. We denote the information extracted from QA pair $k$ as $(p_k, v_k, a_k)$, where $p_k$ is the property mentioned in the question (or $Null$ if there isn't one), $v_k$ is an optional property value mentioned in the question and $a_k$ is the answer, which is a property value.

ASP is then used to conduct the optimisation. One ASP program is constructed for each object type. We outline the ASP program for the object type $t_i$ in the following five stages:
\begin{enumerate}
  \item Firstly, the search space for the optimisation is constructed using choice rules. For each cluster identifier $c_j$, the following two choice rules are added:
  \begin{gather}
    1\{ colour\_mapping(c_j, col_1) ; ... ; colour\_mapping(c_j, col_n) \}1. \\
    1\{ rotation\_mapping(c_j, rot_1) ; ... ; rotation\_mapping(c_j, rot_m) \}1.
  \end{gather}
  \textit{Where the set of possible colour values is: $\{col_1,...,col_n\}$ and the set of possible rotation values is: $\{rot_1,...,rot_m\}$.}

  These choice rules generate one answer set for each possible combination of property-value pairs. Notice that no attempt has been made to restrict the property-value pairs to those which were used to select the number of clusters, since the ASP optimisation will naturally choose the property-value pairs which answer the most questions correctly.

  \item Secondly, for each QA pair, the object data from the frame given in the question is added to the program. Only the objects with type $t_i$ are added. For example, if a question says ``What colour was the leftward-facing fish in frame 23?" and three fish have been detected in frame 23, then only the information for those three fish are listed in the program (fish from other questions will of course be listed in the same program).

  Each of the objects to be added will be given an integer identifier, this identifier is unrelated to the identifier assigned by the tracking component. Each object is also listed with the cluster identifier that it was assigned to. For each object listed in QA pair \textit{$<$k$>$}, which is part of cluster $c_i$ and has identifier $id$, the following predicate is added to the ASP program:
  \begin{equation}
    obj(id, c_i, \textit{$<$k$>$}).
  \end{equation}

  \item A number of helper ASP rules which convert object data in the form of the $obj$ and $\textit{$<$prop$>$}\_mapping$ predicates into the $holds$ predicate, which is used to answer the questions. These two helper rules are as follows:
  \begin{gather}
    holds(colour(\mathit{Val}, \mathit{Id}), Q) :- \begin{multlined}[t]
      obj(\mathit{Id}, Cluster, Q), \\
      colour\_mapping(Cluster, \mathit{Val}).
    \end{multlined} \label{rule:holds-colour} \\
    holds(rotation(\mathit{Val}, \mathit{Id}), Q) :- \begin{multlined}[t]
      obj(\mathit{Id}, Cluster, Q), \\
      rotation\_mapping(Cluster, \mathit{Val}).
    \end{multlined} \label{rule:holds-rotation}
  \end{gather}

  Rules~\ref{rule:holds-colour} and~\ref{rule:holds-rotation} collate the data from the object's cluster and from the property value that that cluster has been assigned and convert this data into $holds$ form. The $holds$ predicate can then be used to answer the questions.

  \item Next, the ASP rules corresponding to the questions and the ASP facts corresponding to the answers to those questions are added to the program. We firstly find the tuples $(p_k, v_k, a_k)$ which correspond to the object type $t_i$ are extracted by the question and answer parsing component from the $\textit{$<$k$>$}^{th}$ QA pair. The following rule and fact are then added if $v_k$ is not $Null$:
  \begin{gather}
    answer(\textit{$<$k$>$}, p_k, V) \text{ :- } \begin{multlined}[t]
      holds(p_{v_k}(v_k, \mathit{Id}), \textit{$<$k$>$}), \\
      holds(p_k(V, \mathit{Id}), \textit{$<$k$>$}), obj(\mathit{Id}, \_, \textit{$<$k$>$}).
    \end{multlined} \\
    answer(\textit{$<$k$>$}, p_{v_k}, V) \text{ :- } \begin{multlined}[t]
      holds(p_{v_k}(V, \mathit{Id}), \textit{$<$k$>$}), \\
      holds(p_k(v_k, \mathit{Id}), \textit{$<$k$>$}), obj(\mathit{Id}, \_, \textit{$<$k$>$}).
    \end{multlined} \\
    expected(\textit{$<$k$>$}, p_k, a_k). \\
    expected(\textit{$<$k$>$}, p_{v_k}, v_k).
  \end{gather}
  \textit{Where $p_{v_k}$ is the property which corresponds to property value $v_k$.}

  For example, the question ``What colour is the downward-facing bag in frame 7?" and answer ``white", would be converted into the following:
  \begin{gather}
    answer(\textit{$<$k$>$}, colour, V) \text{ :- } \begin{multlined}[t]
      holds(rotation(down, \mathit{Id}), \textit{$<$k$>$}), \\
      holds(colour(V, \mathit{Id}), \textit{$<$k$>$}), obj(\mathit{Id}, \_, \textit{$<$k$>$}).
    \end{multlined} \\
    answer(\textit{$<$k$>$}, rotation, V) \text{ :- } \begin{multlined}[t]
      holds(rotation(V, \mathit{Id}), \textit{$<$k$>$}), \\
      holds(colour(white, \mathit{Id}), \textit{$<$k$>$}), obj(\mathit{Id}, \_, \textit{$<$k$>$}).
    \end{multlined} \\
    expected(\textit{$<$k$>$}, colour, white). \\
    expected(\textit{$<$k$>$}, rotation, down).
  \end{gather}

  Two rules are created for questions where $v_k$ is not $Null$ because the question gives away two pieces of information: the colour and the rotation. However, when $v_k$ is $Null$, the following ASP rule and fact are used instead:
  \begin{gather}
    answer(\textit{$<$k$>$}, p_k, V) \text{ :- } holds(p_k(V, \mathit{Id}), \textit{$<$k$>$}), obj(\mathit{Id}, \_, \textit{$<$k$>$}). \\
    expected(\textit{$<$k$>$}, p_k, a_k).
  \end{gather}

  \item The final part of the ASP program is the set of weak constraints used to find the optimal mapping. We firstly define the helper rule, $mapping$, which is used to collate both the colour and rotation property values for each cluster:
  \begin{equation}
    mapping(\mathit{C}, \mathit{Col}, \mathit{Rot}) \text{ :- } \begin{multlined}[t]
      colour\_mapping(\mathit{C}, \mathit{Col}), \\
      rotation\_mapping(\mathit{C}, \mathit{Rot}).
    \end{multlined}
  \end{equation}

  The weak constraints are then defined as follows:
  \begin{gather}
    \text{:$\sim$ } answer(Q, \mathit{Prop}, \mathit{Val}), expected(Q, \mathit{Prop}, \mathit{Val}). [-1@2, Q, \mathit{Prop}, \mathit{Val}] \label{rule:answer-constraint} \\
    \text{:$\sim$ } \begin{multlined}[t]
      mapping(\mathit{C1}, \mathit{Col}, \mathit{Rot}), mapping(\mathit{C2}, \mathit{Col}, \mathit{Rot}), \\
      \mathit{C1} \text{ != } \mathit{C2}. [1@1, \mathit{C1}, \mathit{C2}, \mathit{Col}, \mathit{Rot}]
    \end{multlined} \label{rule:mapping-constraint}
  \end{gather}

  The body of Rule~\ref{rule:answer-constraint} is satisfied when question $Q$ is answered correctly. Since we are looking to maximise the number of questions answered correctly and ASP always minimises weak constraints, we give this rule a negative weight. This rule is given the higher priority of the two.

  Rule~\ref{rule:mapping-constraint}, on the other hand, says that we prefer answer sets where mappings are unique. This rule could also be used as a hard constraint to rule out any answer sets where the mappings are not unique. However, we choose not to enforce this constraint so that the ASP optimiser could choose an answer set where more questions are answered correctly, at the expense of non-unique mappings. The reason for this decision is the following:
  \begin{displayquote}
    If a group of objects with the same property values is split between two or more clusters, and these objects are commonly asked about in the QA pairs, then the ASP optimiser has the ability to assign multiple object clusters to the same property values, if it leads to a larger number of questions being answered correctly.
  \end{displayquote}
  We therefore always try to ensure whatever rules the model learns it learns them in order to maximise the number of questions answered correctly. Hence, Rule~\ref{rule:answer-constraint} is given the highest priority.
\end{enumerate}

The ASP program is run for each object type $t_i$, and $\textit{$<$prop$>$}_mapping$ predicates contained in the optimal answer set in each case are then used to find the optimal mapping $f_{t_i}^*$, which is then stored and used to label all objects in the training data.


\subsection{Data Labelling}
\label{subsection:trained-data-labelling}

Once $f_{t_i}^*$ has been found for every object type $t_i$, all of the objects in the training data can be labelled with property values. Labelling an object's property values requires that the centre point of each cluster be computed. For the sake of efficiency, the cluster centres are precomputed for each object type. The centre for a cluster $c_j$ is computed as the average of the object latent vectors assigned to $c_j$.

After clusters centres have been computed, we assign property values to an object $obj_k$ with type $t_i$ as follows:
\begin{enumerate}
  \item The object's image is encoded into a latent vector, $v_k$, using the autoencoder.
  \item For each cluster $c_j$ which corresponds to object type $t_i$, we compute the cosine distance between $v$ and the centre of $c_j$. The object is assigned to the cluster with the smallest distance, which we denote $c^*$.
  \item Using the mapping found by the ASP optimisation for object type $t_i$, we can simply look up the optimal set of property values that correspond to $c^*$.
\end{enumerate}

Using this method, property values can be assigned to all detected objects in the training videos using only each object's image. After property values have been assigned, we train the property component in exactly the same way as Chapter~\ref{chapter:hardcoded}.

Although the method for training the property component outlined in this section works well when objects are simple and uniform, it is unlikely to scale to more complex, \textit{real-world} datasets. This is because, when training an autoencoder in an unsupervised way, it can learn to extract a lot of noise from the images. When the latent vectors of object images are noisy, clustering will not be as successful in splitting the objects into groups based on their property values.

The speed of the optimisation may also be a concern if the number of property values is large, since the size of the search space scales exponentionally with the average number of property values. However, for the purposes of the OceanQA dataset neither of these potential drawbacks causes any problems. The details of the evaluation of the trained property component, along with the entire trained model, is available in Chapter~\ref{chapter:evaluation}.


\section{Relations}

Unlike the relations component in the hardcoded model, which used a manually engineered algorithm, the relations component in the trained model must learn definitions of binary relations between objects from data. As with the trained properties component outlined above, the relations component uses the QA-data version of the OceanQA dataset. Before the relations component is trained, the trained properties component is used to label all of the objects in the training data with property values.

Instead of using manually engineered functions, we opt to use a neural network consisting of fully-connected layers as the core implementation of the relations component. For a generic environment, one neural network would be created for each binary relation, however, since OceanQA only has one relation, only one network is required for this component.
Each of these networks is assigned a binary relation, $r$, to learn. Each network therefore takes a pair of objects as input, and learns to classify the objects as either being related by $r$ or not.

Since fully-connected layers work with numbers, rather than symbolic data, we need to find a way to encode a pair of objects into a vector. To do this we encode each object in the pair separately (an example encoding of an object is shown in Figure~\ref{fig:encoding}) and then concatenate the two encodings together. The encoding of an object is the result of concatentating each of the following:
\begin{enumerate}
  \item A one-hot encoding of the object's type.
  \item A one-hot encoding of object's colour.
  \item A one-hot encoding of object's rotation.
  \item The position tuple for the object, where each coordinate has been divided by 256 to produce a number of between $0$ and $1$.
\end{enumerate}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{encoding}
  \caption{The vector encoding for a blue, downward-facing octopus, roughly in position $(54, 187, 70, 203)$. In the component itself the position is given to a much higher degree of accuracy than shown here.}
  \label{fig:encoding}
\end{figure}

The neural network takes the vector encoding of the pair of objects as input and passes this vector through a series of fully-connected layers. The details of the layers are shown in Table~\ref{table:relations-network}. As shown in the table, the output of the network is a single neuron. The sigmoid function is applied to the output of the network to ensure the value is between $0$ and $1$. If the value of the output of a network learning relation $r$ is denoted $o_r$, then the relation classifier component's output for a pair of objects, denoted $r(\mathit{obj1}, \mathit{obj2})$, is as follows:
\begin{equation}
  r(\mathit{obj1}, \mathit{obj2}) =
  \left\{
  	\begin{array}{ll}
  		true  & \mbox{if } o_r \geq 0.5 \\
  		false & \mbox{otherwise}
  	\end{array}
  \right.
  \label{eqn:trained-close-def}
\end{equation}

\begin{table}[b]
  \centering
  \begin{tabular}{ |c|c c| }
    \hline
    \textbf{Layer} & \textbf{Input Size} & \textbf{Output Size} \\
    \hline
    Fully-connected 1 & 38   & 1024 \\
    Fully-connected 2 & 1024 & 256  \\
    Fully-connected 3 & 256  & 64   \\
    Fully-connected 4 & 64   & 1    \\
    \hline
  \end{tabular}
  \caption{Details of the layers of each relation classification network. The input is the size of two encoded objects, and the output is used for binary classification. Dropout of $0.2$ is also applied between each pair of layers.}
  \label{table:relations-network}
\end{table}

Since the OceanQA dataset has only a single binary relation, \textit{close}, only one neural network is required to be trained for the relation component. Training this network is fairly straightforward; the process is comprised of the following steps:
\begin{enumerate}
  \item All of the relation questions in the QA-data version of the training dataset are collated.
  \item From each QA pair, a frame number, two objects (including types and, optionally, property values) and the answer are extracted using the question-and-answer parsing component.
  \item The frame number and object information is used to look up any missing property values for each object. The property component has already been applied to the dataset, so the property values of all detected objects are guaranteed to be labelled. The full set of property values, as well as the object type and position from the detector, is used to construct the object encoding. The two encodings are concatenated to produce the object-pair encoding.
  \item Finally, the set of object-pair encodings, and their associated answers, is used to train the network corresponding to the \textit{close} relation.
\end{enumerate}

After the network has been trained, the component can be used to classifier binary relations between objects. When being applied to a given video, the relations component classifies every pair of objects in every frame of the video as being either close or not using Equation~\ref{eqn:trained-close-def}. If two objects in frame $\textit{$<$frame$>$}$ with identifiers \textit{$<$id1$>$} and \textit{$<$id2$>$}, respectively, are deemed to be close by the component, then the $obs(close(\textit{$<$id1$>$}, \textit{$<$id2$>$}), \textit{$<$frame$>$})$ predicate is stored. For any pair of objects deemed to not be close, nothing is stored; the closed-world assumption (CWA) - that is, that any predicate which cannot be proved to be true is false - is made for relations between objects.

Applying the network to every pair of objects in every frame of a video could become a burden on the speed of the relations component, especially when there are many objects in each frame. Batching pairs of objects together when applying them to the network helps to improve efficiency, but it has its limits. Chapter~\ref{chapter:evaluation} shows that the relations component is the second slowest, after the detector, of all the components in the trained model. However, for the purposes of this project, the relations component is fast enough. Chapter~\ref{chapter:evaluation} also shows the full details of this components performance with perfect inputs.


\section{Events}

The final of the three core components in the trained model is the event component. As with the properties and relations component, the event component uses the QA-data version of the dataset for training. However, unlike the previous components, both the properties and relations components are applied to the training data in order to extract relevant symbolic information, prior to training the event component. During training, the event component can therefore access data that has been extracted by all preceeding components in the architecture: the object detector, properties, tracking and relations components.

\pagebreak

Since a lot of symbolic information will already have been extracted from the video during evaluation by preceeding components, the event component does not need to look at raw pixels in the video at all. Instead, it is much more efficient to use the data that has already been gleaned from the video to work out which events occur. Unlike the hardcoded events component, the trained component cannot assume to have access to an $\mathcal{AL}$ model of the environment. Instead, it must learn its own symbolic rules using the training data. One option is to learn an $\mathcal{AL}$ model and use this to find which events occur in the same way as the hardcoded component. However, these $\mathcal{AL}$ rules can be quite complex, so we do not consider this option for this project. Instead, we opt to learn ASP rules which, given a symbolic enoding for a pair of frames, return the action which occurs between the two frames. Once the actions for the entire video have been found, additional ASP rules can be used to find the effects.


\subsection{Terminology}

The ASP rules for finding the actions which occur in a video are found using a greedy search algorithm. The algorithm is run three times; once for each each action. Before outlining this algorithm, we define the terminology used as follows:
\begin{itemize}
  \item \textbf{Feature} :- High-level names, whose values categorise the state of an object. We make use of the following features: $x$-position, $y$-position, colour, rotation and disappear (defined below). Each object has a specific value for each feature, which come (directly or indirectly) from the information extracted from the video.

  \item \textbf{Feature Sort} :- Each feature can be categorised into the following three \textit{sorts}, based on the set of possible values that an object can hold for that feature:
  \begin{enumerate}
    \item \textbf{Binary}. Disappear is the only binary feature.
    \item \textbf{Discrete} (excluding binary). Colour and rotation are discrete features.
    \item \textbf{Continuous}. $x$-position and $y$-position are both continuous features.
  \end{enumerate}

  \item \textbf{Feature Operation} :- An operation that is applied to the value of a feature. Each feature operation is usually intended to be applied to a single feature \textit{sort}. For example, a discrete feature operation cannot be applied to continuous feature values.

  \item \textbf{Operation Result} :- The result of applying a feature operation to a feature value. Every operation result must be a binary value, since we want to use these values in the body of ASP rules.
\end{itemize}

The goal of the training algorithm is, given a dataset $\{(F_i, F_{i+1}, a_i)\}$, where $F_i$ and $F_{i+1}$ is the set of feature values for frame $i$ and $i+1$, respectively, and $a_i$ is the action which occurs between frame $i$ and $i+1$, find a set of rules with operation results in the body which best explain the data. In this case, explaining the data means maximising the number of questions answered correctly, where each contains a frame $i$ (using which we can look up $F_i$ and $F_{i+1}$ in the extracted data) and an answer (which provides $a_i$).

As mentioned above, each feature operation is associated with a feature sort. For each sort, we now define the set of possible operations that can be applied to feature values of that sort. For every object in the video, a feature operation $\sigma$ is applied to the pair $(f_{v_{i}}, f_{v_{i+1}})$, where $f_{v_i}$ and $f_{v_{i+1}}$ are the object's values for feature $f$ in frame $i$ and $i+1$, respectively. Every set of feature operations also includes the $\top$ operation, which always evaluates to true.

Firstly, for binary features we only consider values in the initial frame, however, it would be very simple to expand this to include both frames. In the following, we show the the set of possible feature operations for a binary feature, $\Sigma_{\mathit{binary}}$. Each operation takes the pair $(f_{v_{i}}, f_{v_{i+1}})$ as input:
\begin{equation}
  \Sigma_{\mathit{binary}} =
  \left\{
  	\begin{array}{ll}
  		\sigma(f_{v_{i}}, f_{v_{i+1}}) := f_{v_i} \\
  		\sigma(f_{v_{i}}, f_{v_{i+1}}) := \neg f_{v_i} \\
      \sigma(f_{v_{i}}, f_{v_{i+1}}) := \top
  	\end{array}
  \right\}
  \label{eqn:trained-close-def}
\end{equation}

Secondly, a pair of discrete feature values $(f_{v_{i}}, f_{v_{i+1}})$ can have the following feature operations applied:
\begin{equation}
  \Sigma_{\mathit{discrete}} =
  \left\{
  	\begin{array}{ll}
  		\sigma(f_{v_{i}}, f_{v_{i+1}}) := f_{v_i} = v_n, f_{v_{i+1}} = v_m \textit{ for each $(v_n, v_m)$} \\
  		\sigma(f_{v_{i}}, f_{v_{i+1}}) := f_{v_i} = f_{v_{i+1}} \\
      \sigma(f_{v_{i}}, f_{v_{i+1}}) := f_{v_i} != f_{v_{i+1}} \\
      \sigma(f_{v_{i}}, f_{v_{i+1}}) := \top
  	\end{array}
  \right\}
  \label{eqn:trained-close-def}
\end{equation}
\textit{Where $(v_n, v_m)$ loops over all possible combinations of feature values.}

Finally, a pair of continuous feature values $(f_{v_{i}}, f_{v_{i+1}})$ can take the following feature operations:
\begin{equation}
  \Sigma_{\mathit{continuous}} =
  \left\{
  	\begin{array}{ll}
  		\sigma(f_{v_{i}}, f_{v_{i+1}}) := f_{v_i} > f_{v_{i+1}} \\
      \sigma(f_{v_{i}}, f_{v_{i+1}}) := f_{v_i} < f_{v_{i+1}} \\
  		\sigma(f_{v_{i}}, f_{v_{i+1}}) := f_{v_i} = f_{v_{i+1}} \\
      \sigma(f_{v_{i}}, f_{v_{i+1}}) := f_{v_i} != f_{v_{i+1}} \\
      \sigma(f_{v_{i}}, f_{v_{i+1}}) := \top
  	\end{array}
  \right\}
  \label{eqn:trained-close-def}
\end{equation}

The $\top$ feature operations always give an operation result of \textit{true}. Intuitively, a $\top$ feature operation has no effect on the outcome of the rule it is in. Since the algorithm has the option to choose the $\top$ feature operation for any feature, we can enforce that all features appear in every rule body at least once. Any feature that the algorithm doesn't care about can be assigned a $\top$ feature operation. For simplicity, we also enforce that each feature can appear at most once in a rule body. The search algorithm therefore tries to learn a set of rules, where each rule takes the following form:
\begin{equation}
  a_i \textbf{ if } \begin{multlined}[t]
    \sigma_{x\_position}(f_{v_{i}}, f_{v_{i+1}}), \sigma_{y\_position}(f_{v_{i}}, f_{v_{i+1}}), \sigma_{colour}(f_{v_{i}}, f_{v_{i+1}}), \\
    \sigma_{rotation}(f_{v_{i}}, f_{v_{i+1}}), \sigma_{disappear}(f_{v_{i}}, f_{v_{i+1}})
  \end{multlined}
  \label{eqn:event-rule-layout}
\end{equation}
\textit{Where each $\sigma_{\textit{$<$f$>$}}$ is a member of \textit{$<$f$>$}'s set of possible feature operations.}

\pagebreak

Before outlining an Inductive Logic Programming (ILP) search algorithm for finding the set of rules, we first give an example which explains the terminology and the construction of the search space.

\textbf{Example.} Consider an example in the dataset which includes an action, \textit{move}, and an octopus which is upward-facing, blue and has position $(100, 40, 116, 56)$ in frame 6, and is upward-facing, purple and has position $(100, 25, 116, 41)$ in frame 7. For simplicity we only consider the $x$-position and $y$-position features. Each position feature only uses the top left corner of the object's bounding box, since both corners move by the same amount. The position feature operations are given as follows:

\smallskip

\begin{minipage}{0.45\textwidth}
  \centering
  $x$-position operations $\rightarrow$ results
  \begin{gather*}
    100 > 100 \rightarrow \mathit{false} \\
    100 < 100 \rightarrow \mathit{false} \\
    100 = 100 \rightarrow \mathit{true} \\
    100 \text{ } != 100 \rightarrow \mathit{false} \\
    \top \rightarrow \mathit{true}
  \end{gather*}
\end{minipage}
\begin{minipage}{0.45\textwidth}
  \centering
  $y$-position operations $\rightarrow$ results
  \begin{gather*}
    40 > 25 \rightarrow \mathit{true} \\
    40 < 25 \rightarrow \mathit{false} \\
    40 = 25 \rightarrow \mathit{false} \\
    40 \text{ } != 25 \rightarrow \mathit{true} \\
    \top \rightarrow \mathit{true}
  \end{gather*}
\end{minipage}

\smallskip

It is easy to see that there are many possible rules which the algorithm could learn, given only this single example, although many of these rules are undesired. However, when there are many examples, the algorithm should be able to learn useful rules. The search space required to learn a single rule using a complete algorithm for this example (showing only the position features) is shown in Figure~\ref{fig:search-space}.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{search-space}
  \caption{Part of the search space for a complete search algorithm, showing only the $x$-position and $y$-position features operations. The full search space would also include rotation, colour and disappear feature operations.}
  \label{fig:search-space}
\end{figure}


\subsection{Search Strategy}

Equation~\ref{eqn:event-rule-layout} shows that, for each rule, the training algorithm needs to search over the set of all possible combinations of feature operations, where each feature is assigned exactly one operation in the rule body. Even for a relatively simple environment, such as OceanQA, this is a very large search space, and when the possibility of searching over an arbitrary number of rules is taken into account the space becomes even larger. The number of feature operations for each feature and the total search space size is shown in Table~\ref{table:feature-op-sizes}

\begin{table}
  \centering
  \begin{tabular}{ |c|c| }
    \hline
    \textbf{Feature} & \textbf{Search Size} \\
    \hline
    $x$-position & 5 \\
    $y$-position & 5 \\
    colour       & 52 \\
    rotation     & 19 \\
    disappear    & 3 \\
    \hline
    \hline
    Total & 74100 \\
    \hline
  \end{tabular}
  \caption{Number of feature operations for each feature. The total search size is then calculated by multiplying all these sizes together.}
  \label{table:feature-op-sizes}
\end{table}

To ensure the training algorithm remains tractable, we implement a greedy search over features, rather than a complete search over the entire space. Our search algorithm takes each feature individually and finds the operation for the given feature which maximises the number of event questions answered correctly.

In the case where only a single rule is to be learnt, our search algorithm takes each feature individually and finds the operation for the given feature which maximises the number of event questions answered correctly. The set of operations that have been selected so far are then stored as the \textit{accumulated operations}. When the next feature is optimised it can choose to either use the full set of stored operations or none - using none means setting all previously optimised feature operations to $\top$. The updated set of \textit{accumulated operations} chosen by the algorithm is now stored. This process continues until all features have been optimised.

The case where the algorithm is left to decide how many rules are required - where the number of rules to learn is part of the search - is more complex. In this case there is one set of accumulated operations for each rule learnt, and, during the next feature optimisation, the algorithm can choose any of the accumulated operation sets for each rule being learnt. Algorithm~\ref{algo:optimise} gives a high-level overview of the steps required to find the optimal hyposthesis - the set of rules which correctly answers the most questions - for each action.

\begin{algorithm}[h!]
  \caption{Finding the optimal hyposthesis for each action}
  \label{algo:optimise}
  \begin{algorithmic}[1]
    \Procedure{OptimiseRules}{$\mathit{actions}, \mathit{features}, data$}
      \State $hyps \gets \{\}$
      \For{$a \gets actions$}
        \State $acc \gets \{\}$
        \For{$f \gets \mathit{features}$}
          \State $s \gets$ \Call{SortOf}{$f$}
          \State $acc \gets $ \Call{OptimiseFeature}{$action, \Sigma_s, acc, data$}
        \EndFor
        \State $hyps \gets hyps + $ \Call{GenHyp}{$acc$}
      \EndFor
      \State \textbf{return} $hyps$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}


\subsection{ASP Encoding}

The \textsc{OptimiseFeature} function shown in Algorithm~\ref{algo:optimise} takes as input an action, a set of feature operations, the accumulated feature operations and the training data for the given action. This function searches over all of the feature operations to find the set of rules which maximise the number of questions answered correctly. It also searches over the number of rules to be used, as well as whether each of those rules should use an accumulated operation set or not. We implement this function using an ASP optimisation program. This section outlines the details of that program.



\end{document}
