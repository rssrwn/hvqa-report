\documentclass[../report.tex]{subfiles}


\begin{document}


\chapter{OceanQA Dataset}
\label{chapter:dataset}

Before discussing the implementation of our solution to the VideoQA task, it is important to outline the data that is used to train and evaluate the model. Rather than using one of the datasets outlined in Chapter~\ref{chapter:related}, we opted to create a new VideoQA dataset, which we name `OceanQA'. While it would have been preferable to use an existing dataset (and an existing implementation as a baseline) to allow a fair comparison, none of the existing datasets suited the project requirements\footnotemark, for the following two reasons:
\begin{enumerate}
  \item Most of the existing VideoQA datasets use videos from real-world environments, where objects and events are usually more complex than computer-generated environments. Training models to work with real-world data therefore requires significant computational resources and can take days or even weeks. Given the time and resource limitations that exist for this project, it was sensible to avoid these datasets. More generally, creating a dataset gives greater flexibility over the size and complexity of the data; if faster training is required, we can simply create smaller images or use fewer objects in each video.

  \item Hybrid models generally require some form of environment specification (or ontology) so that the model's internal knowledge can be represented explicitly (see~\cite{dataset:clevrer, model:ns-cl, model:ns-vqa} for examples from VideoQA and VQA). Since most of the existing VideoQA datasets do not limit objects to be of specific types, or restrict object properties or video events to a given set, they cannot provide such a specification of the environment.
\end{enumerate}

\footnotetext{The CLEVRER dataset~\cite{dataset:clevrer} meets these requirements and would have been a good candidate for this project. Unfortunately, it was published in March 2020, six months after the project began.}

The dataset is generated programmatically and can therefore be made as large as required. However, in order to allow comparisons between models, we generate a fixed dataset of 1400 videos (each video contains 10 question-answer pairs) and use this data to train and evaluate the models outlined in subsequent Chapters. This dataset is divided into 1000 training, 200 validation and 200 testing videos.

The generated dataset can be used in either `full-data' form or in `QA-data' form. The full-data form contains videos, question-answer pairs and the ground truth of all the information in the videos; this means that every object property, relation and event is labelled by the dataset. This form gives the programmer complete access to the information in the video, which allows baseline models to be constructed, and may also allow internal parts of models to be evaluated. The QA-form, on the other hand, contains only videos and question-answer pairs. Since no additional data about the video is provided, this form reflects a `real' VideoQA dataset, and should be used for evaluating the model as a whole.

Since the focus of this project is to investigate logical reasoning, we do not attempt to make the job of the neural network difficult by creating complex scenes; the dataset emulates a simple retro-game environment. Each image is also quite small at 256x256 pixels to allow faster network training. The remainder of this Chapter outlines the full details of the OceanQA dataset.


\section{Videos}
\label{section:dataset-videos}

% Talk about objects, properties, relations, actions and events.

Each video in the OceanQA dataset is a sequence of 32 frames. Each frame contains a flat background and a maximum of 16 objects. Objects form the central component of each video, since all of the useful information in each video can be modelled by the following: properties of objects; binary relations between objects; and events, which relate to at least one object, occuring between two consecutive frames of the video. Each object is modelled using the following attributes: object type (or class), position, rotation and colour.

Figure~\ref{fig:dataset-objects} shows an example of each of the four possible classes of objects. Each class can be described as follows:
\begin{itemize}
  \item \textbf{Octopus}. The `main' character in the video - the octopus is the only non-static character and its properties change due to its actions. Each frame contains at most one octopus. The initial frame always contains a red octopus with a randomly assigned rotation.

  \item \textbf{Fish}. Fish are always silver, but can have any rotation. When the octopus comes close to the fish, the fish disappears (gets eaten).

  \item \textbf{Bag}. Similarly to fish, plastic bags are always white but can take any rotation. Bags are harmful to the octopus, so both objects disappear when close.

  \item \textbf{Rock}. Rocks can have four colours: brown, blue, purple and green, but always face upright. When an octopus comes near a rock the octopus' colour will change (if necessary) to match that of the rock (it will be camouflaged).
\end{itemize}

\begin{figure}[t]
  \begin{subfigure}{0.24\textwidth}
    \centering
    \includegraphics[width=\textwidth]{octopus.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.24\textwidth}
    \centering
    \includegraphics[width=\textwidth]{fish.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.24\textwidth}
    \centering
    \includegraphics[width=\textwidth]{bag.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.24\textwidth}
    \centering
    \includegraphics[width=\textwidth]{rock.png}
  \end{subfigure}
  \caption{Examples of each object type in the videos (not to scale).}
  \label{fig:dataset-objects}
\end{figure}

Objects in each frame are always enclosed by a rectangular box. Object positions are given as $(x1, y1, x2, y2)$, where $(x1, y1)$ is the top left corner of the object, and $(x2, y2)$ is the bottom right\footnote{The $y$ (vertical) direction is downward increasing.}.

The dataset models a single binary relation between objects, `close to'. Internally this relation is defined as: object A is close to object B if, after expanding A by 5 pixels on each side, for each pair of parallel edges of A (edges in the horizontal direction and edges in the vertical direction), one of the edges either overlaps with B or is fully contained within B. The `close to' relation is symmetric. The algorithm for determining closeness is outlined in pseudocode in Algorithm~\ref{algo:close_to}. The dataset does not consider any relations with an arity higher than two.

\begin{algorithm}[h]
  \caption{Determine whether one object is close to another}
  \label{algo:close_to}
  \begin{algorithmic}[1]
    \Procedure{CloseTo}{$obj1, obj2$}
      \State \Call{ExpandEdges}{$obj1$}\Comment{Add $5$ pixels to each side}
      \State $overlapX \gets obj2.x2 \geq obj1.x2 \textbf{ and } obj2.x1 \leq obj1.x1$
      \State $overlapY \gets obj2.y2 \geq obj1.y2 \textbf{ and } obj2.y1 \leq obj1.y1$
      \For{$(x,y) \gets obj2.corners$}
        \State $matchX \gets obj1.x1 \leq x \leq obj1.x2 \textbf{ or } overlapX$
        \State $matchY \gets obj1.y1 \leq y \leq obj1.y2 \textbf{ or } overlapY$
        \If{$matchX \textbf{ and } matchY$}
          \State \textbf{return} $True$
        \EndIf
      \EndFor
      \State \textbf{return} $False$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

Other than object properties and relations between objects, which encode visual and spatial information from a single frame, the other key data from the video are the events, which encode temporal information from the frames. Events occur between two consecutive frames, and each timestep can contain multiple (or no) events. Each video therefore contains 31 sets of events.

We choose to split events into two disjoint sets: actions and effects. Actions can be thought of as motions that an object can make to alter its position or rotation. We consider three such actions: move, rotate clockwise and rotate anticlockwise. Rotations have the intuitive effect on an object's rotation. Move causes the object to move 15 pixels in the direction of its rotation. For example, an object at position $(20, 100, 30, 110)$ with a `right-facing' rotation will be at position $(35, 100, 45, 110)$ after a move action.

Effects, on the other hand, are direct consequences of actions. Effects always alter an object's state (its class, position, rotation and colour). The dataset contains three effects: change colour, eat a fish and eat a bag. The octopus is the only object which can change colour, and this only occurs when the octopus is close to a rock. As described above, the octopus takes the colour of the rock. The octopus then continues to keep this new colour, even after moving away from the rock. Figure~\ref{fig:dataset-frames} shows a snippet from a video where an octopus moves close to a rock and changes colour. A fish or bag is eaten when an octopus moves close, this causes the fish or bag to disappear. However, unlike a fish, when a bag is eaten the octopus also disappears. Examples of all events can be found in Appendix~\ref{appendix-dataset}.

To create a video, we first create an initial frame by randomly sampling the number of each object from a set of uniform distributions: $\mathcal{U}(5, 8)$ for fish, $\mathcal{U}(2, 3)$ for bags and $\mathcal{U}(3, 4)$ for rocks, where $\mathcal{U}(l, u)$ refers to a discrete uniform distribution with lower and upper bounds $l$ and $u$, respectively. Where colours and rotations need to be chosen for objects, these are sampled uniformly from the set of colours and/or rotations that are possible for that type of object. To create the rest of the video, actions for the octopus are randomly sampled for each timestep. We then work out which effects have occurred in each frame and update the properties of each object accordingly. The octopus has a $0.1$ probability of choosing to rotate at each timestep. However, if the octopus chooses to move but moving would cause the octopus to be outside the frame, the octopus will rotate instead. Clockwise and anticlockwise rotations are sampled uniformly.

\begin{figure}[t]
  \begin{subfigure}{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{frame_0.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{frame_1.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{frame_2.png}
  \end{subfigure}
  \caption{An example of an octopus moving close to a green rock and turning green.}
  \label{fig:dataset-frames}
\end{figure}


\section{Questions and Answers}

As well as videos, VideoQA datasets must also contain question-answer pairs (QA pairs). Answers to questions `label' part of the video. For example, if a question asks the model to find the event which occurs between two frames, the answer to that question labels the event. However, any event not associated with a QA pair is unlabelled. The shortage of labels is not specific to events; there can be as many as 512 object instances in a single video, but perhaps only a single QA pair will label an object with a property value. Relations between objects face the same problem. For this reason, VideoQA datasets, unlike many other supervised learning datasets, can contain a lot of sparsely labelled data. On one hand, this may be beneficial to the model, since it only needs to understand the parts of the video which are mentioned in the questions, but, on the other, it can hinder the training of the model, since there is less data than would otherwise be available. Chapter~\ref{chapter:trained} offers one solution to this problem.

The OceanQA dataset contains ten QA pairs per video, sampled randomly from seven question-types. The remainder of this section introduces each question type separately, along with a grammar for each question and answer presented in extended Backus-Naur form (EBNF). Since Natural Language Processing (NLP) is not the focus of this project, we choose to use a small, discrete set of structured question templates, which allow the model to easily extract relevant symbolic information from the questions. Allowing free-form natural langauge questions creates additional complexity and uncertainty for the model, which we felt would distract from the core focus on hybrid machine learning for VideoQA. Generating these free-form questions can also be very time and resource intensive. A further discussion on possible future work on hybrid models for VideoQA tasks which use free-form questions and answers is contained in Chapter~\ref{chapter:conclusion}.

Equation~\ref{eqn:spec-grammar} formalises the grammar of the dataset's objects, relations and events, which was implicitly described in Section~\ref{section:dataset-videos}. These EBNF grammar rules are used in the rules for the questions and answers.

\begin{equation}
  \label{eqn:spec-grammar}
  \begin{split}
    \textit{$<$object$>$} & ::= [ \textit{$<$property\_value$>$} ] \textit{  $<$class$>$ } \\
    \textit{$<$property\_value$>$} & ::= \textit{$<$rotation\_value$>$ } | \textit{ $<$colour\_value$>$ } \\
    \textit{$<$rotation\_value$>$} & ::= \text{upward-facing } | \text{ right-facing } | \text{ downward-facing } | \text{ left-facing } \\
    \textit{$<$colour\_value$>$} & ::= \text{red } | \text{ blue } | \text{ purple } | \text{ brown } | \text{ green } | \text{ silver } | \text{ white } \\
    \textit{$<$class$>$} & ::= \text{octopus } | \text{ fish } | \text{ bag } | \text{ rock } \\
    \textit{$<$property$>$} & ::= \text{rotation } | \text{ colour } \\
    \textit{$<$relation$>$} & ::= \text{close} \\
    \textit{$<$event$>$} & ::= \textit{$<$action$>$ } | \textit{ $<$effect$>$ } \\
    \textit{$<$action$>$} & ::= \text{move } |  \text{ rotate clockwise } | \text{ rotate anti-clockwise } \\
    \textit{$<$effect$>$} & ::= \text{change colour } | \text{ eat a fish } | \text{ eat a bag } \\
    \textit{$<$frame\_idx$>$} & ::= \text{$0$ } | \text{ $1$ } | \text{ ... } | \text{ $31$ }
  \end{split}
\end{equation}

As mentioned above there are seven question types. The first two of these are VQA questions, which only require the model to look at a single frame of the video. The remaining five question types require the model to reason across frames. We adapt the repetition count, repeating action and state transition questions from~\cite{dataset:tgif-qa} (discussed in Chapter~\ref{chapter:related}) to the OceanQA environment. We also add two further video-specific questions: questions about actions between two frames and questions about changing property values. The full set of question and answer templates is as follows:

\begin{enumerate}
  \item Property questions are designed to test a model's understanding of object properties. In the training data $41\%$ of the questions ask about colour, while $59\%$ ask about rotation. All property values are represented in the training data, but not uniformly; some property values are very scarce. This reflects the underlying scarcity of objects with particular property values in the dataset; green octopuses, for example, are quite rare, while silver fish are very common. The EBNF grammar for property questions and answers is as follows:
  \begin{equation}
    \begin{split}
      \textit{$<$q\_type\_1$>$} & ::= \text{What \textit{$<$property$>$} was the \textit{$<$object$>$} in frame \textit{$<$frame\_idx$>$}?} \\
      \textit{$<$ans\_type\_1$>$} & ::= \textit{$<$property\_value$>$}
    \end{split}
  \end{equation}

  \item Relation questions test a model's understanding of binary relations between objects. These questions only require a yes-or-no answer. The answer is `yes' for roughly $16\%$ of these questions in the training data. This imbalance reflects the lack of instances of binary relations between objects, since we only model the `close' relation. Objects are selected randomly from a random frame of the video. The grammar for these questions and answers is as follows:
  \begin{equation}
    \begin{split}
      \textit{$<$q\_type\_2$>$} & ::= \text{Was the \textit{$<$object$>$} \textit{$<$relation$>$} to the \textit{$<$object$>$} in frame \textit{$<$frame idx$>$}?} \\
      \textit{$<$ans\_type\_2$>$} & ::= \text{yes } | \text{ no}
    \end{split}
  \end{equation}

  \item Action questions ask which action occurred between two frames of a video. `Move' actions account for around $45\%$ of answers to these questions, while `rotate clockwise' and `rotate anticlockwise' account for approximately $27.5\%$ of questions each. The answer to these questions will never be `nothing'. The grammar for action questions and answers is as follows:
  \begin{equation}
    \begin{split}
      \textit{$<$q\_type\_3$>$} & ::= \text{Which action occurred immediately after frame \textit{$<$frame\_idx$>$}?} \\
      \textit{$<$ans\_type\_3$>$} & ::= \textit{$<$action$>$}
    \end{split}
  \end{equation}

  \item Changed-property questions require the model to reason about how a property of the octopus changes from one frame to the next. The dataset guarantees that only a single (explicit) property changes immediately after \textit{$<$frame\_idx$>$}. Approximately $78\%$ of these questions ask about the colour of the octopus, while the remaining $22\%$ ask about the rotation. The grammar is as follows:
  \begin{equation}
    \begin{split}
      \textit{$<$q\_type\_4$>$} & ::= \text{What happened to the octopus immediately after \textit{$<$frame\_idx$>$?}} \\
      \textit{$<$ans\_type\_4$>$} & ::= \text{Its \textit{$<$property$>$} changed from \textit{$<$property\_value$>$} to \textit{$<$property\_value$>$}}
    \end{split}
  \end{equation}

  \item Repetition count questions ask the model to work out how many times an event occurs in a given video. This requires the model to be able to count the number of occurences of an event. Events are sampled from a uniform distribution; if the event never occurs in the video, the answer is simply $0$. Since each event can occur at most once per frame-interval, the answer is guaranteed to be between 0 and 30 (inclusive). The grammar for repetition count questions and answers is as follows:
  \begin{equation}
    \begin{split}
      \textit{$<$q\_type\_5$>$} & ::= \text{How many times does the octopus \textit{$<$event$>$}?} \\
      \textit{$<$ans\_type\_5$>$} & ::= \text{$0$ } | \text{ $1$ } | \text{ ... } | \text{ $30$ }
    \end{split}
  \end{equation}

  \item Repeating action questions are similar to repetition count questions, but instead of asking the model for a number they ask the model to find the event which occurs a given number of times. Events cannot be sampled uniformly since, in a given video, multiple events may have the same count. Approximately $86\%$ of questions are about actions, while the remaining $14\%$ refer to effects. This imbalance is again due to the underlying scarcity of particular events in the dataset. There is a unique answer to every question. The grammar for repeating action questions and answers is as follows:
  \begin{equation}
    \begin{split}
      \textit{$<$q\_type\_6$>$} & ::= \text{What does the octopus do \textit{$<$positive\_int$>$} times?} \\
      \textit{$<$ans\_type\_6$>$} & ::= \textit{$<$event$>$} \\
      \textit{$<$positive\_int$>$} & ::= \text{$0$ } | \text{ $1$ } | \text{ ... } | \text{ $30$ }
    \end{split}
  \end{equation}

  \item State transition questions ask the model to find the action that occurs after a given event. $78\%$ of the answers to these questions refer to the `move' action, while `rotate clockwise' and `rotate anticlockwise' are the answers to $11\%$ of the questions each. In addition to asking the model to reason temporally about actions and events, these questions also require the model to understand which instance of an event is the `nth' occurrence. The `nth time' section of the grammar is unused if there is only one occurence of the event in the video. The grammar is as follows:
  \begin{equation}
    \begin{split}
      \textit{$<$q\_type\_7$>$} & ::= \begin{multlined}[t]
          \text{What does the octopus do immediately after} \\
          \text{\textit{$<$event\_noun$>$} [for the \textit{$<$nth$>$} time]?}
        \end{multlined} \\
      \textit{$<$ans\_type\_7$>$} & ::= \textit{$<$action$>$} \\
      \textit{$<$event\_noun$>$} & ::= \begin{multlined}[t]
          \text{rotating clockwise } | \text{ rotating anticlockwise } | \\
          \text{eating a fish } | \text{ eating a bag } | \text{ changing colour}
        \end{multlined} \\
      \textit{$<$nth$>$} & ::= \text{first } | \text{ second } | \text{ third } | \text{ fourth } | \text{ fifth}
    \end{split}
  \end{equation}
\end{enumerate}

Property, relation and action questions are intended to be used to train and test the model's understanding of object properties, binary relations between objects and actions between two consecutive frames, respectively. Since the property and relation questions contain the `\textit{$<$object$>$}' rule, knowledge of property values may be required to select the corresponding object from the frame. This means it would be helpful to train the model's understanding of object properties before relations. Selecting the training data for object properties is, however, non-trivial. Chapter~\ref{chapter:trained} discusses one possible solution to this problem. The final four question types are included to diversify the data the model can be trained on and to evaluate how well the model can combine its understanding of object properties, relations and events. None of the questions require any sort of external knowledge (other than the ability to count).

Questions are generated independently for each video, and questions are sampled uniformly from the seven question templates given above. This leads to an approximately uniform distribution of question types in the datase, as shown in Figure~\ref{fig:q_types}. Since there are ten questions in each video the training, validation and testing datasets contain $10000$, $2000$ and $2000$ questions, respectively. Appendix~\ref{appendix-dataset} contains a number of example images and QA pairs from the dataset.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{question_type_dist.png}
  \caption{The number of QA pairs in the training data for each question type.}
  \label{fig:q_types}
\end{figure}


\section{Specification}

% Outline a formal specificiation and why it is necessary/helpful.

Unlike end-to-end neural network models, hybrid models require knowledge that has been extracted from a video to be made explicit. For example, if a neural model is shown a video and asked for the colour of the octopus, it will first extract features from the video and then encode the video into a vector. This vector is combined with the encoding of the question, before a final section of the model produces either a short, free-form sentence or a probability distribution over possible answers. Relevant information from the video, including the colour of the octopus, is implicitly included in the vector encoding of the video. Hybrid models, however, usually require that this information is symbolic so that logical reasoning can be employed to find an answer to the question. For this reason it is necessary to define a specification of the video environment, which outlines the set of concepts which the model must learn in order to accurately answer the questions. This specification can also provide a way to inject background knowledge about the environment into the model.

Each environment specification must contain the following information:
\begin{enumerate}
  \item The number of frames in each video. This project assumes this to be fixed, but relaxing this assumption would not require any major changes to the construction of the dataset or the model outlined in the rest of this report.

  \item A set of pairs, $\langle \textit{class}, \textit{is\_static} \rangle$. Each element of the set contains the type of an object and a boolean corresponding to whether the object is capable of performing an action. All (relevant) object classes must be mentioned in this set.

  \item A set of pairs, $\langle \textit{property}, \{ \textit{ property\_value } \} \rangle$, corresponding to a set of properties along with a set of their respective values. This representation for object properties assumes that each property has a discrete, finite set of values. Continuous properties are not considered in this project. Object class and position are assumed to be implicit properties since all objects must have a value for them. These implicit properties should not be listed here.

  \item A set of binary relations.

  \item A set of actions.

  \item A set of effects of actions.
\end{enumerate}

Although it has been described in detail already, Equation~\ref{eqn:env-spec} provides the formal environment specification for the OceanQA dataset.

\begin{equation}
  \label{eqn:env-spec}
  \begin{split}
    \textit{frames} & ::= 32 \\
    \textit{objects} & ::= \{ \langle \text{octopus}, \text{ false} \rangle, \langle \text{fish}, \text{ true} \rangle, \langle \text{bag}, \text{ true} \rangle, \langle \text{rock}, \text{ true} \rangle \} \\
    \textit{properties} & ::= \begin{multlined}[t]
        \{ \langle \text{colour}, \{ \text{red}, \text{ blue}, \text{ purple}, \text{ brown}, \text{ green}, \text{ silver}, \text{ white} \} \rangle \\
        \langle \text{rotation}, \{ \text{upward-facing}, \text{ right-facing}, \text{ downward-facing}, \text{ left-facing} \} \rangle \}
      \end{multlined} \\
    \textit{relations} & ::= \{ \text{close} \} \\
    \textit{actions} & ::= \{ \text{move}, \text{ rotate clockwise}, \text{ rotate anticlockwise} \} \\
    \textit{effects} & ::= \{ \text{change colour}, \text{ eat a fish}, \text{ eat a bag} \}
  \end{split}
\end{equation}

As alluded to already, an environment specification, like the one shown in Equation~\ref{eqn:env-spec}, requires that objects, properties, relations, actions and effects are all discrete. Continuous variables could be modelled by discretising, but this may lead to lower accuracy and a large number of possible values. This may have to be treated as a trade-off for using hybrid models. On the other hand, this is one of the first pieces of work which explores the use of hybrid models in VideoQA tasks; future work may be able to overcome this problem.


\end{document}
