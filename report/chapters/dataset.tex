\documentclass[../report.tex]{subfiles}


\begin{document}


\chapter{Dataset}
\label{chapter:dataset}

Before discussing the implementation of our solution to the VideoQA task, it is important to outline the data that is used to train and evaluate the model. Rather than using one of the datasets outlined in Chapter~\ref{chapter:related}, we opted to create a new VideoQA dataset, which we name TODO. While it would have been preferable to use an existing dataset (and an existing implementation as a baseline) to allow a fair comparison, none of the existing datasets suited the project requirements\footnotemark, for the following reasons:
\begin{enumerate}
  \item Most of the existing VideoQA datasets use videos from real-world environments, where objects and events are usually more complex than computer-generated environments. Training models to work with real-world data therefore requires significant computational resources and can take days or even weeks. Given the time and resource limitations that exist for this project, it was sensible to avoid these datasets. More generally, creating a dataset gives greater flexibility over the size and complexity of the data; if faster training is required, we can simply create smaller images or use fewer objects in each video.

  \item Hybrid models generally require some form of environment specification (or ontology) so that the model's internal knowledge can be represented explicitly (see~\cite{dataset:clevrer, model:ns-cl, model:ns-vqa} for examples from VideoQA and VQA). Since most of the existing VideoQA datasets do not limit objects to be of specific types, or restrict object properties or video events to a given set, they cannot provide such a specification of the environment.
\end{enumerate}

\footnotetext{The CLEVRER dataset~\cite{dataset:clevrer} meets these requirements and would have been a good candidate for this project. Unfortunately, it was published in March 2020, six months after the project began.}

The dataset is generated programmatically, and can therefore be made as large as required. However, in order to allow comparisons between models, we generate a fixed dataset of 1400 videos (each video contains 10 question-answer pairs) and use this data to train and evaluate the models outlined in Chapter~\ref{chapter:implementation}. This dataset is further divided into 1000 training, 200 validation and 200 testing videos.

Since the focus of this project is to investigate logical reasoning, we do not attempt to make the job of the neural network difficult by creating complex scenes; the dataset emulates a simple retro-game environment. Each image is also quite small at 256x256 pixels to allow faster network training. The remainder of this chapter outlines the full details of the TODO dataset.


\section{Videos}
\label{section:dataset-videos}

% Talk about objects, properties, relations, actions and events.

Each video in the TODO dataset is a sequence of 32 frames. Each frame contains a flat background and a maximum of 16 objects. Objects form the central component of each video, since all of the useful information in each video can be modelled by the following: properties of objects; binary relations between objects; and events, which relate to at least one object, occuring between two consecutive frames of the video. Each object is modelled using the following attributes: object type (or class), position, rotation and colour.

Figure~\ref{fig:dataset-objects} shows an example of each of the four possible classes of objects. Each class can be described as follows:
\begin{itemize}
  \item \textbf{Octopus}. The `main' character in the video - the octopus is the only non-static character and its properties change due to its actions. Each frame contains at most one octopus. The initial frame always contains a red octopus with a randomly assigned rotation.

  \item \textbf{Fish}. Fish are always silver, but can have any rotation. When the octopus comes close to the fish, the fish disappears (gets eaten).

  \item \textbf{Bag}. Similarly to fish, plastic bags are always white but can take any rotation. Bags are harmful to the octopus, so both objects disappear when close.

  \item \textbf{Rock}. Rocks can have four colours: brown, blue, purple and green, but always face upright. When an octopus comes near a rock the octopus' colour will change (if necessary) to match that of the rock (it will be camouflaged).
\end{itemize}

\begin{figure}[b!]
  \begin{subfigure}{0.24\textwidth}
    \centering
    \includegraphics[width=\textwidth]{octopus.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.24\textwidth}
    \centering
    \includegraphics[width=\textwidth]{fish.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.24\textwidth}
    \centering
    \includegraphics[width=\textwidth]{bag.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.24\textwidth}
    \centering
    \includegraphics[width=\textwidth]{rock.png}
  \end{subfigure}
  \caption{Examples of each object type in the videos (not to scale).}
  \label{fig:dataset-objects}
\end{figure}

The dataset models a single binary relation between objects, `close to'. Internally this relation is defined as: object A is close to object B if, after expanding A by 5 pixels on each side, for each pair of parallel edges of A (edges in the horizontal direction and edges in the vertical direction), one of the edges either overlaps with B or is fully contained within B. The `close to' relation is symmetric. The algorithm for determining closeness is outlined in pseudocode in Algorithm~\ref{algo:close_to}. Object positions are always given as $(x1, y1, x2, y2)$, where $(x1, y1)$ is the top-left corner of the object, and $(x2, y2)$ is the bottom right. The dataset does not consider any relations with an arity higher than two.

\begin{algorithm}[t]
  \caption{Determine whether one object is close to another}
  \label{algo:close_to}
  \begin{algorithmic}[1]
    \Procedure{CloseTo}{$obj1, obj2$}
      \State \Call{ExpandEdges}{$obj1$}\Comment{Add $5$ pixels to each side}
      \State $overlapX \gets obj2.x2 \geq obj1.x2 \textbf{ and } obj2.x1 \leq obj1.x1$
      \State $overlapY \gets obj2.y2 \geq obj1.y2 \textbf{ and } obj2.y1 \leq obj1.y1$
      \For{$(x,y) \gets obj2.corners$}
        \State $matchX \gets obj1.x1 \leq x \leq obj1.x2 \textbf{ or } overlapX$
        \State $matchY \gets obj1.y1 \leq y \leq obj1.y2 \textbf{ or } overlapY$
        \If{$matchX \textbf{ and } matchY$}
          \State \textbf{return} $True$
        \EndIf
      \EndFor
      \State \textbf{return} $False$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

Other than object properties and relations between objects, which encode visual and spatial information from a single frame, the other key data from the video are the events, which encode temporal information from the frames. Events occur between two consecutive frames, and each timestep can contain multiple (or no) events. Each video therefore contains 31 sets of events.

We choose to split events into two disjoint sets: actions and effects. Actions can be thought of as motions that an object can make to alter its position or rotation. We consider three such actions: move, rotate clockwise and rotate anticlockwise. Rotations have the intuitive effect on an object's rotation. Move causes the object to move 15 pixels in the direction of its rotation. For example, an object at position $(20, 100, 30, 110)$ with a `right-facing' rotation will be at position $(35, 100, 45, 110)$ after a move action.

Effects, on the other hand, are direct consequences of actions. Effects always alter an object's state (its class, position, rotation and colour). The dataset contains three effects: change colour, eat a fish and eat a bag. The octopus is the only object which can change colour, and this only occurs when the octopus is close to a rock. As described above the octopus takes the colour of the rock. The octopus then continues to keep this colour, even after moving away from the rock. Figure~\ref{fig:dataset-frames} shows a snippet from a video where an octopus moves close to a rock and changes colour. A fish or bag is eaten when an octopus moves close, this causes the fish or bag to disappear. However, when a bag is eaten the octopus also disappears, this does not occur with fish. Examples of all events can be found in Appendix TODO.

To create a video, we first create an initial frame by randomly sampling the number of each object from a set of uniform distributions: $\mathcal{U}(5, 8)$ for fish, $\mathcal{U}(2, 3)$ for bags and $\mathcal{U}(3, 4)$ for rocks, where $\mathcal{U}(l, u)$ refers to a discrete uniform distribution with lower and upper bounds $l$ and $u$, respectively. Where colours and rotations need to be chosen for objects, these are sampled uniformly from the set of colours and/or rotations that are possible for that type of object.

To create the rest of the video, actions for the octopus are randomly sampled for each timestep, we then work out which effects have occurred in each frame and, finally, the properties of each object are updated accordingly. The octopus has a $0.1$ probability of choosing to rotate at each timestep. However, if the octopus chooses to move but moving would cause the octopus to be outside the frame, the octopus will rotate instead. Clockwise and anticlockwise rotations are sampled uniformly.

\begin{figure}[t]
  \begin{subfigure}{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{frame_0.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{frame_1.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{frame_2.png}
  \end{subfigure}
  \caption{An example of an octopus moving close to a green rock and turning green.}
  \label{fig:dataset-frames}
\end{figure}


\section{Questions and Answers}

As well as videos, VideoQA datasets must also contain question-answer pairs (QA pairs). Answers to questions are usually the only form of direct supervison given to a VideoQA model, and, unlike other supervised learning datasets, VideoQA datasets can contain lots of sparsely labelled data. For example, our dataset can contain 31 octopus actions per video, but only a small number of these may be `labelled' by QA pairs. Properties of objects are even more sparsely labelled, since there can be as many as 512 object instances in a single video, but perhaps only a single QA pair. For this reason, VideoQA tasks can resemble `semi-supervised' tasks, where a model is given partially labelled data, but asked to produce all labels.

The TODO dataset contains ten QA pairs per video, sampled randomly from seven question-types. The remainder of this section introduces each question type separately, along with a grammar for each question and answer presented in extended Backus-Naur form (EBNF). Equation~\ref{eqn:spec-grammar} formalises the grammar of the dataset's objects, relations and events, which was implicitly described in Section~\ref{section:dataset-videos}. These EBNF grammar rules are used in the rules for the questions and answers.

\begin{equation}
  \label{eqn:spec-grammar}
  \begin{split}
    \textit{$<$object$>$} & ::= [ \textit{$<$property\_value$>$} ] \textit{  $<$class$>$ } \\
    \textit{$<$property\_value$>$} & ::= \textit{$<$rotation\_value$>$ } | \textit{ $<$colour\_value$>$ } \\
    \textit{$<$rotation\_value$>$} & ::= \text{upward-facing } | \text{ right-facing } | \text{ downward-facing } | \text{ left-facing } \\
    \textit{$<$colour\_value$>$} & ::= \text{red } | \text{ blue } | \text{ purple } | \text{ brown } | \text{ green } | \text{ silver } | \text{ white } \\
    \textit{$<$class$>$} & ::= \text{octopus } | \text{ fish } | \text{ bag } | \text{ rock } \\
    \textit{$<$property$>$} & ::= \text{rotation } | \text{ colour } \\
    \textit{$<$relation$>$} & ::= \text{close} \\
    \textit{$<$event$>$} & ::= \textit{$<$action$>$ } | \textit{ $<$effect$>$ } \\
    \textit{$<$action$>$} & ::= \text{move } |  \text{ rotate clockwise } | \text{ rotate anti-clockwise } \\
    \textit{$<$effect$>$} & ::= \text{change colour } | \text{ eat a fish } | \text{ eat a bag } \\
    \textit{$<$frame\_idx$>$} & ::= \text{$0$ } | \text{ $1$ } | \text{ ... } | \text{ $31$ }
  \end{split}
\end{equation}

As mentioned above there are seven question types. The first three of these are intended to be used to train and test the model's understanding of specific pieces of information contained in a video. The first three question types correspond to understanding of object properties, relations between objects and events between frames, respectively. Each of these questions requires only knowledge of the preceeding question types to answer. TODO rewrite this if a new question type is added. For example, event questions require knowledge of both object properties and relations, but relation questions only require knowledge of object properties. The final four question types are included to diversify the data the model can be trained on and to evaluate how well the model can combine its understanding of object properties, relations and events. These questions do not require knowledge outside of these three areas.

\begin{enumerate}
  \item Property questions are designed to test a model's understanding of object properties. It does not require knowledge of any other part of the video to answer, however knowledge of object properties may be required to select the correct training data (this is discussed further in Chapter~\ref{chapter:hybrid-video-qa}). TODO talk about distribution of answers. The EBNF grammar for property questions and answers is as follows:
  \begin{equation}
    \begin{split}
      \textit{$<$q\_type\_1$>$} & ::= \text{What \textit{$<$property$>$} was the \textit{$<$object$>$} in frame \textit{$<$frame\_idx$>$}?} \\
      \textit{$<$ans\_type\_1$>$} & ::= \textit{$<$property\_value$>$}
    \end{split}
  \end{equation}

  \item Relation questions test a model's understanding of binary relations between objects. These questions only require a yes-or-no answer, and the answer is `yes' for roughly TODO \% of these questions. The grammar for these questions and answers is as follows:
  \begin{equation}
    \begin{split}
      \textit{$<$q\_type\_2$>$} & ::= \text{Was the \textit{$<$object$>$} \textit{$<$relation$>$} to the \textit{$<$object$>$} in frame \textit{$<$frame idx$>$}?} \\
      \textit{$<$ans\_type\_2$>$} & ::= \text{yes } | \text{ no}
    \end{split}
  \end{equation}

  \item Action questions ask which action occurred between two frames of a video. TODO talk about distribution of actions in the answers.
  \begin{equation}
    \begin{split}
      \textit{$<$q\_type\_3$>$} & ::= \text{Which action occurred immediately after frame \textit{$<$frame\_idx$>$}?} \\
      \textit{$<$ans\_type\_3$>$} & ::= \textit{$<$action$>$}
    \end{split}
  \end{equation}
\end{enumerate}


\section{Specification}

Outline a formal specificiation and why it is necessary/helpful.



\end{document}
