\documentclass[../report.tex]{subfiles}


\begin{document}


\chapter{Evaluation}
\label{chapter:evaluation}

% TODO object tracker evaluation - perfect performance, if not remove ref from end of h-perl chapter (discuss that object tracker is not officially since there are no questions for it, but that unofficially it achieves perfect performance). Under actual object detector performance conditions as well.

% Talk about timings, including trained relation component (or else remove ref from end of relations in trained model chapter).


This chapter describes the performance details of the components and models outlined in Chapters~\ref{chapter:hardcoded} and~\ref{chapter:trained}. Section~\ref{section:eval-components} compares the performance of a selection of counterpart components used in the hardcoded and trained models. Section~\ref{section:eval-models} compares the overall performance of the two H-PERL models, and includes details of the models' performance under noisy conditions.

Before presenting the evaluation, we outline the following three key criteria that we want to evaluate the models against:
\begin{enumerate}
  \item \textbf{Accuracy} :- What proportion of questions does the model answer correctly?
  \item \textbf{Speed} :- How long does the model take to complete the evaluation?
  \item \textbf{Adaptability} :- How simple is it to transfer the model to a new environment?
\end{enumerate}

While we would have preferred to have compared our models to state-of-the-art neural network implementations for VideoQA, training end-to-end VideoQA networks is very resource intensive and takes a long time. If these models were included in the evaluation, we would add explainability as an additional criterion. However, since our models are both hybrid, their implementations have roughly the same explainability. We do, however, keep in mind that hybrid models are often easier to understand than fully-neural counterparts, as well as being significantly faster to train.


\section{Components}
\label{section:eval-components}

The first part of the evaluation concerns the individual model components. In this section we compare the performance of the properties, relations and events components from both the hardcoded and trained models. These components are evaluated using perfect input data - where we assume no mistakes have been made in previous parts of the pipeline. We also outline the performance of the object detection component, which is common to both H-PERL models. All of these components are evaluated using the 200 validation videos, rather than the testing videos, although both sets of videos are generated in the same way.

This section does not include a dedicated discussion on the object tracker's performance, since there are no QA pairs with which it can be directly evaluated. However, after using some intuitive heuristics to gauge the component's object tracking abilities, we see that the tracker assigns identifiers in exactly the way we would expect for the OceanQA environment. In the rest of this chapter, therefore, it should be assumed that the tracker operates with perfect accuracy on the OceanQA environment, but we cannot provide an official evaluation for this.


\subsection{Object Detector}

In Chapter~\ref{chapter:background} we mentioned that \textit{Intersection over Union} (IoU), which is calculated between a ground truth bounding box and the bounding box produced by the detector, measures how well a detector localises an image. As a reminder, the IoU between an object A and object B is defined as the area of intersection between A and B, divided by the area of union between A and B. Chapter~\ref{chapter:background} also mentioned that a threshold $t$ can be applied to a set of detections so that only the detections with confidence $> t$ are used. Each IoU threshold that is applied to the set of detections produces a different precision-recall curve for the detector. The average precision (AP) is then defined as the area under the precision-recall curve, although estimates of this value are used. Average precision, however, is only defined for detection of a single class. The mean average precision (mAP) metric is therefore used to find the mean AP across $k$ classes, and is intuitively defined as follows:
\begin{equation}
  mAP = \frac{\sum_{i=1}^{k} AP_i}{k}
\end{equation}

In order to conduct the evaluation, we used the \textit{PyCocoTools}\footnote{Available at: https://github.com/cocodataset/cocoapi} library, which implements the metrics used in the COCO object detection challenge. The library provides mAP values for IoU thresholds $0.5$ and $0.75$, as well as the average mAP over the set of thresholds $\{ 0.5, 0.55, 0.6, ..., 0.95 \}$. Table~\ref{table:detector-maps} presents mAP values achieved by our detector on the OceanQA validation dataset.

\begin{table}[h]
  \centering
  \begin{tabular}{ |c|c| }
    \hline
    \textbf{IoU Threshold} & \textbf{mAP} \\
    \hline
    0.5        & 1.000 \\
    0.75       & 0.998 \\
    0.5 : 0.95 & 0.995 \\
    \hline
  \end{tabular}
  \caption{Mean average precision values for a number of intersection over union thresholds. A threshold of 0.5~:~0.95 indicates an average mAP value for the set of thresholds: $\{ 0.5, 0.55, ..., 0.95 \}$.}
  \label{table:detector-maps}
\end{table}

The results presented in Table~\ref{table:detector-maps} show that our object detector performs very strongly. This isn't particularly surprising, since the dataset environment is very simple and the object detection algorithm used is state-of-the-art. Nonetheless it proves the assumption that it is possible, in the OceanQA environment at least, to extract objects from the frames of the video and work with these objects directly.

Despite the simpicity of the environment, the object detector is, however, quite slow. Detection of all objects in the testing dataset takes about 56 seconds; making the detector the slowest component in the trained model, and the second slowest in the hardcoded model. The full details of the timings of all components are outlined in TODO. Since the object detector can be trained for a specific environment, we consider it to be adaptable. Training can take a long time however, and each environment must have a large number of object detection examples available.


\subsection{Properties}




\subsection{Relations}


\subsection{Events}


\section{Models}
\label{section:eval-models}

% TODO When timings are created update refs above.



\end{document}
