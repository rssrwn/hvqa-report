\documentclass[../report.tex]{subfiles}


\begin{document}


\chapter{Hybrid Property, Event and Relation Learner}
\chaptermark{H-PERL}
\label{chapter:h-perl}

We outline a novel paradigm for solving the VideoQA problem. This approach merges deep learning and logic-based machine learning and inference methods in an attempt to learn the concepts required to answer the questions. We name this approach: Hybrid Property, Event and Relation Learner (H-PERL). Section~\ref{section:h-perl-model} outlines the structure of an H-PERL model, while Section~\ref{section:common-components} discusses the implementation of a number of H-PERL components which are common to all models presented in the subsequent chapters.


\section{H-PERL Model}
\label{section:h-perl-model}

% Introduce Hybrid VideoQA Model architecture. Talk about how this model models an environment using objects, properties, relations, actions and events. (show generic architecture diagram)

H-PERL is a generic, pipelined architecture for VideoQA tasks. The pipeline is composed of a number of components which, when strung together, form a model. A model is, therefore, a set of component implementations, and can be thought of as an `instance' of H-PERL. Chapters~\ref{chapter:hardcoded} and~\ref{chapter:trained} each outline an H-PERL model for the OceanQA dataset.


\subsection{Architecture}

The H-PERL architecture assumes that all of the information in an environment which is required to answer the questions can be modeled using: objects; binary relations between objects; and events (which occur due to an object) between two consecutive frames in the video. For many environments, simple environments (like our OceanQA dataset) in particular, this assumption holds. However, for many VideoQA datasets, particularly those set in the real-world, this assumption may not be suitable. For example, extracting some objects from a video may be non-trivial (as in the case of abstract nouns), and yet information on these objects may still be required to answer the questions. Additionally, H-PERL is not capable of modelling relations between objects with an arity larger than 2.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{h-perl-pipeline}
  \caption{An overview of the H-PERL architecture for VideoQA. Green, red and blue shading indicates components which work to extract features at different levels of abstraction. Grey shading indicates the `core' components of the architecture.}
  \label{fig:h-perl-pipeline}
\end{figure}

Figure~\ref{fig:h-perl-pipeline} shows the components involved in a typical H-PERL model. During evaluation, information from the video and the question flow, from left to right, through the pipeline. Each H-PERL model assumes that the object detection, question parsing and QA system components are ``pre-made'' (either pre-trained or manually engineered). We refer to these as `non-core' components. H-PERL allows the remaining `core' components to be updated as the model is trained, although they don't necessarily have to be. Each core component in the pipeline accumulates information. This means that each component guarantees that existing information (or features of the data) will not be overwritten (with the small exception of the event component when error correction is used, discussed further in Chapter~\ref{chapter:hardcoded}). As shown in Figure~\ref{fig:h-perl-pipeline} components in the pipeline work at different levels of abstraction; the object properties and tracking components work to extract object-level features, while the relations and events components work to extract frame and video-level features, respectively.

The following is a high-level description of the tasks each component is required to complete for the H-PERL architecture to work with high accuracy:
\begin{enumerate}
  \item \textbf{Question Parser}. The QA parsing component is used to extract relevant pieces of information from the questions (and answers when training). For example, given the question: ``What does the octopus do immediately after eating a fish?" and that the question is a state-transition question, the parsing component would extract, firstly, that the object in question was an octopus, and secondly, that the event was `eat a fish'. The parsing component, therefore, bridges the gap between the symbolic data, which the model works with, and the natural language questions and answers. When an H-PERL model is being evaluated, only the question needs to be parsed. However, when the model is training this component acts as a question-and-answer parser, since the model requires that the feedback that comes from the answer is also in symbolic form.

  \item \textbf{Object Detector}. The detection component produces bounding boxes and classes for each object in each frame of the video. Any object not detected at this stage of the pipeline is assumed to be part of the background and is therefore ignored by the rest of the model.

  \item \textbf{Property Extractor}. Given a set of images of objects from the detection component, the property extraction component assigns a value to every property listed in the environment specification for every object in the set.

  \item \textbf{Object Tracker}. The object tracker is required to assign an identifier to each object in a given video. The object identifiers assigned in the initial frame of the video can be arbitrary, but then an algorithm is usually applied inductively to each remaining frame of the video in an attempt to assign each object the same identifier as it was given in the previous frame.

  \item \textbf{Relation Classifier}. The job of the binary relation classifier is, given a symbolic representation of a video, to list all of the instances of binary relations between objects in the video. The set of possible binary relation is defined in the dataset's environment specification.

  \item \textbf{Event Detector}. The event detection component produces a set of events for each pair of consecutive frames in the video. Each set of events can contain both actions and effects, and each event consists of an event name and an object identifier which signifies the non-static object that took part in that event.

  \item \textbf{QA System}. The job of the QA system is to take all of the features which have been accumulated by previous components in the pipeline, along with a parsed question, and produce an answer to the question. The QA system is also given the question type, this allows it to make sense of the parsed question and to apply different reasoning for each question type.
\end{enumerate}

While VideoQA tasks may require the pipeline to contain the above set of components, the H-PERL architecture can also be modified for solving related problems. For example, by removing the tracking and events components and keeping the rest of the pipeline the same, a modified version of the H-PERL architecture could be used to solve VQA (also known as FrameQA) tasks. In fact, the architecture could be modified to work with many different input types, including images, videos, text or speech, provided that the data contains something akin to an `object', and that extracting these objects (and features from these objects) is feasible. We could also swap out the final component in the pipeline, the QA system, for another task-specific component. For example, we could create a video (or image) captioning architecture by using a captioning component rather than a QA component. Figure~\ref{fig:other-pipelines} shows a number of architectures similar to H-PERL that have been modified for other tasks.

\begin{figure}[h!]
  \begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{vqa-pipeline}
    \caption{An architecture for a VQA task.}
  \end{subfigure}
  \begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{captioning-pipeline}
    \caption{An architecture for an image captioning task.}
  \end{subfigure}
  \caption{Potential modifications to the H-PERL architecture for solving other tasks. Grey shading indicates feature extraction components. Green and red shading indicate components which extract object-level and frame-level features, respectively.}
  \label{fig:other-pipelines}
\end{figure}

At a high level, then, the H-PERL approach consists of three main stages:
\begin{enumerate}
  \item Attention restriction
  \item Feature extraction
  \item Task-specific output generation
\end{enumerate}

These stages are similar to some approaches used in VQA tasks~\cite{attention-vqa, stacked-attention-vqa}, where the model's attention is firstly restricted to specific parts of the input (in this case objects), before features are extracted from the input and an output is generated. However, unlike other approaches, H-PERL does not dynamically update the model's attention based on the input question. Another key difference between H-PERL and other question-answering approaches is that extracted features are stored symbolically. This gives the observer a better understanding of what the model is learning and allows the injection of background or commonsense knowledge directly into the model.


\subsection{Information Representation}

The first component in the H-PERL pipeline, the object detector, takes a raw video as input and produces a set of bounding boxes (object positions) and object classes for each frame of the video. After object detection has been applied, data about the video is stored symbolically for each object in each frame of the video. Each of the subsequent components in the pipeline can access this symbolic data along with the raw image of each object (the raw video frames are discarded). Each of these components then accumulates symbolic information about the objects, frames or video. This is what is meant by extracting object, frame and video-level features.

Once the symbolic features have been extracted from the input, each of the components in the architecture need an agreed upon way of representing the information. This symbolic representation is as follows:
\begin{itemize}
  \item For an object with identifier \textit{$<$id$>$} in frame \textit{$<$frame$>$}, the object's properties, rotation and class (all referred to as $\textit{$<$property$>$}$), each with value $\textit{$<$value$>$}$, are represented as follows:
  \begin{equation}
    \text{obs}(\textit{$<$property$>$}(\textit{$<$value$>$}, \textit{$<$id$>$}), \textit{$<$frame$>$})
  \end{equation}
  As described in Chapter~\ref{chapter:dataset}, the value of an object's rotation is given as $(x1, y1, x2, y2)$, where $(x1, y1)$ is the top left corner of the object, and $(x2, y2)$ is the bottom right.

  \item For two objects in frame \textit{$<$frame$>$}, with identifiers \textit{$<$id1$>$} and \textit{$<$id2$>$}, a binary relation, \textit{$<$relation$>$}, between the objects is represented as follows:
  \begin{equation}
    \text{obs}(\textit{$<$relation$>$}(\textit{$<$id1$>$}, \textit{$<$id2$>$}), \textit{$<$frame$>$})
  \end{equation}

  \item An event which occurs immediately after frame \textit{$<$frame$>$}, due to an object with identifier \textit{$<$id$>$}, is represented by one of the following rules, depending on whether the event was an action or effect:
  \begin{equation}
    \text{occurs\_action}(\textit{$<$action$>$}(\textit{$<$id$>$}), \textit{$<$frame$>$})
  \end{equation}
\end{itemize}

Using these representations an entire video can be encoded into a logic program. The final component in the pipeline, the QA system, takes as input a video encoding and a set of parsed questions, and constructs a set of ASP rules which are used, along with the video encoding, to find an answer to the question.


\subsection{Requirements}

To give a complete view of what is needed to construct an H-PERL model, we outline the minimum set of requirements and a number of assumptions. Each instance of an H-PERL model may require additional constraints to be applied on top of these. The set of requirements is as follows:
\begin{enumerate}
  \item A set of pre-made, non-core components (question parser, object detector and QA system).

  \item A VideoQA dataset, where each element is of the form: $$\langle \text{video}, \{ \langle \text{question}, \text{answer}, \text{question type} \rangle \} \rangle$$

  \item Environment specification for the given dataset.

  \item (Optionally) Background knowledge of the environment, written in ASP.
\end{enumerate}


H-PERL also requires that the following assumptions be made about the data:
\begin{enumerate}
  \item All relevant information in the video can be modelled by object properties; binary relations between objects; and events occuring between consecutive frames of the video.

  \item Each of the properties, relations and events components can be trained individually and directly using a specific type of question. For example, for the OceanQA dataset, we can use question types 1, 2 and 3 to train the properties, relations and events components, respectively.

  \item As mentioned in Chapter~\ref{chapter:dataset}, properties, relations and events must be discrete. There is also currently no way of modelling continuous variables in the data; for example, we cannot say that the octopus rotated clockwise by $\frac{1}{4}\pi$ radians.
\end{enumerate}

These requirements and assumptions clarify some of the limitations of H-PERL models. Firstly, for some environments pre-trained object detectors (or data to train them with) may be difficult to find. Secondly, it may also be difficult, if not impossible, to construct or train a question parser for free-form, natural language question-answering datasets. Additionally, most QA datasets are not guaranteed to contain questions which can be used to directly train components of the model. Finally, many environments will simply be too complex to be accurately modelled by discrete properties, relations and events. These requirements and assumptions do, however, provide initial directions for future research in the area of Hybrid question-answering models. We discuss some potential extensions to H-PERL in Chapter~\ref{chapter:conclusion}.

As well as drawbacks, the H-PERL architecture does also provide a number of advantages. One of the most important advantages of hybrid models is the ability to encode commonsense knowledge or background knowledge of the environment directly into the model without having to learn it. Since the model accumulates video information symbolically, we can inject background knowledge at any stage of the pipeline. A number of further advantages of hybrid models are presented in the subsequent chapters, and these, along with the disadvantages, are summarised in Chapter~\ref{chapter:conclusion}.


\section{Common Components}
\label{section:common-components}

The two models, one hardcoded and one trained, which are outlined in subsequent chapters, contain a number of common components. Before describing the implementation of these models, we first outline the details of the components common to both. These components include the non-core components, which the H-PERL pipeline assumes are pre-made, as well as the object tracker, which uses the a similar algorithm for both the hardcoded and trained models.


\subsection{Question Parser}

As mentioned previously, the role of the question parsing component is to translate the natual language questions into a set of keywords (and their types). This extracted information is then used by the QA system to construct ASP rules with which the answers to the questions are found. During training, the question parser acts as a question-and-answer parser, as the model needs to collect training data from the answer.

Both the hardcoded and the trained H-PERL models use a very simple hand-engineered question parser. Since the question parser is told the type of the question it has been given, and since the OceanQA dataset uses templated questions, the question parser can simply extract the relevant parts of the question by looking at the corresponding template. This is implemented by splitting the question into a list of words and simply picking the correct word based on the template.

Take the question ``What does the octopus do immediately after rotating clockwise for the second time?" as an example. Based on the question template shown in Chapter~\ref{chapter:dataset}, the parsing component would extract the following from the question:
\begin{equation}
  \begin{split}
    \textit{object} & = \text{octopus} \\
    \textit{action} & = \text{rotate clockwise} \\
    \textit{occurrence} & = \text{2}
  \end{split}
\end{equation}
Note that the action noun `rotating clockwise' has been converted to the verb form, as given in the environment specification. In addition, the occurrence, `second', is converted to a number, which is simpler for the QA system to work with. These conversions are all hand-engineered, as opposed to learnt.

The parsing of all other question types is done in the exactly same way. For the sake of brevity these are not shown in this discussion. The discussion on the QA system in Section~\ref{subsection:qa-system} does, however, give more detail. As a general rule, the information extracted from the questions is the parts of the question templates which are contained in $<>$ brackets. For question types 4, 5, 6 and 7 the parser must also extract the \textit{object} keyword, which is guaranteed to be octopus in all four of these question types.


\subsection{Object Detector}

Chapter~\ref{chapter:background} describes two object detection algorithms: Faster R-CNN and YOLO. Faster R-CNN has been shown to be the more accurate of the two~\cite{cnn-uses:yolo-v3}. YOLO's key advantage, however, is its speed; it has been shown to be capable of processing five times as many frames per second as Faster R-CNN on the Pascal VOC dataset~\cite{yolo}. However, since OceanQA frames are fairly small and simple, and since we don't need real-time performance for VideoQA tasks, we prefer accuracy over speed and therefore choose Faster R-CNN over YOLO for the object detection component.

The object detection network uses a pre-built Faster R-CNN model from the \textit{Torchvision}\footnote{Available at: https://github.com/pytorch/vision} library. The network is trained on the full-data OceanQA dataset (where every object in every frame is fully labelled, as outlined in Chapter~\ref{chapter:dataset}), using Faster R-CNN's multi-task loss function~\cite{cnn-uses:faster-r-cnn}. The \textit{PyCOCOTools}\footnote{Available at: https://github.com/cocodataset/cocoapi} library is used to implement the core training functionality.

As is standard in Faster R-CNN implementations, we provide the detection model with a feature extraction network. This network's role is to extract features from the input frames and pass these to the pre-built detection network. We provide a three-layer convolutional network, described in Table~\ref{table:object-detection-layers}, with randomly initialised weights. These weights, along with the weights of the detection network, are updated as the whole model is trained.

An example visualisation of the detector's output is shown in Figure~\ref{fig:detection-example}. The full details of the object detector's performance are given in Chapter~\ref{chapter:evaluation}.

\begin{center}
\begin{threeparttable}
  \begin{tabular}{ |l|c c c c c| }
    \hline
    \textbf{Layer} & \textbf{In Channels} & \textbf{Out Channels} & \textbf{Kernel Size} & \textbf{Stride} & \textbf{Padding} \\
    \hline
    Conv1 & 3   & 32   & 3x3  & 1  & 1 \\
    Conv2 & 32  & 64   & 3x3  & 2  & 1 \\
    Conv3 & 64  & 128  & 3x3  & 2  & 1 \\
    \hline
  \end{tabular}
  \caption{Specification of the feature extraction network. Batch normalisation is applied between layers Conv1 and Conv2, and between layers Conv2 and Conv3.}
  \label{table:object-detection-layers}
\end{threeparttable}
\end{center}

\begin{figure}
  \centering
  \includegraphics[width=0.6\textwidth]{detection-example.png}
  \caption{A visualisation of the output of the detector on an example frame.}
  \label{fig:detection-example}
\end{figure}


\subsection{Object Tracker}

The object tracker's task is to assign every object in the video with an identifier. Ideally, an object is assigned the same identifier throughout the video, however, errors in the object detection can make this difficult. The tracker is given a video containing a list of frames, with each frame containing a set of detected objects (each object contains just a class and a position at this stage).

We implement a simple object tracker which assigns integer identifiers, starting from zero, to each object in the initial frame of the video. The tracker then inductively applies an algorithm (discussed below) which attempts to match objects in the previous frame (we call these previous objects), which have been assigned an identifier, with objects in the next frame (next objects), which are unidentified. An object, $obj1$, matches with another object, $obj2$, if the following two conditions are met:
\begin{enumerate}
  \item $obj1$ and $obj2$ have the same class.
  \item The Euclidean distance between the top left corner of $obj1$ and the top left corner of $obj2$ is no more than 30 pixels.
\end{enumerate}
Both the distance metric and the maximum distance value are considered hyperparameters of the tracking component. Many other values for these hyperparameters would work equally well with the OceanQA dataset, and if these values weren't known in advance, they could be found fairly easily through a combination of analysis of the dataset (the maximum distance must be at least 15 pixels) and trial-and-error.

After assigning identifiers to objects in the initial frame, the tracker iteratively applies the following algorithm to each subsequent frame:
\begin{enumerate}
  \item Each object in the new frame votes for the object in the previous frame which matches with it best.

  \item Each previous object collates all of the new objects which have voted for it and chooses the new object with which it best matches.

  \item If a previous object does not get any votes, it is because there are no new objects which consider this object their best match. We therefore assume the previous object has disappeared. Each disappeared object is added to a \textit{hidden objects} list, so that if the object reappeared in a subsequent frame the same identifier could be assigned.

  \item Each remaining previous object is now matched with a single new object. The new object is assigned the identifier of the previous object it is matched with.

  \item Some new objects - namely those which were rejected by a previous object in favour of another new object - remain unidentified. Since all previous objects have either passed on their identifier or disappeared, these remaining objects must be new. For each of these new objects, if it cannot be matched with an object in the \textit{hidden objects} list, a previously unused identifier is created and assigned to the object, otherwise the object is assigned its best match in the \textit{hidden objects} list.

  \item Each object in the \textit{hidden objects} list can only stay for a maximum of five frames from when they are added (although this duration is also considered a hyperparameter and could therefore be altered). The final step of the algorithm is to remove from this list any hidden object which has overstayed its welcome.
\end{enumerate}

This simple, heuristic-based tracker works well for simple, well-defined environments such as OceanQA. In fact, Chapter~\ref{chapter:evaluation} shows that the tracker can achieve perfect performance, assuming the object detection is completely accurate. However, this tracker may perform poorly in more complex environments, such as real-world VideoQA datasets. Other, more advanced object tracking algorithms may fare better, however. The Simple Online and Realtime Tracking (SORT)~\cite{sort-obj-tracking} algorithm is one of the best performers~\cite{obj-tracking-survey}. Since the H-PERL architecture is completely modular, our tracking algorithm could simply be swapped out for a more advanced alternative when the environment required it.


\subsection{QA System}
\label{subsection:qa-system}

As shown in Section~\ref{section:h-perl-model}, the information extracted from the video by the core components of the H-PERL architecture is stored symbolically. The role of the QA system is then, given these symbolic features and a set of parsed questions and their respective types, to find an answer to each question.

Since the features are stored as ASP predicates, a simple implementation of this component is to construct a set of ASP rules for each question type, and use these rules to find the questions' answers. To construct these rules we use a set of templates, one for each question type. After a question has been parsed, we fill in the question's corresponding template using information extracted by the question parsing component.

Since there are ten questions for each video, the QA system can save time by answering all ten questions in the same ASP program. To do this a set of ASP question-answering rules is generated for each question. Each set of rules contains at least one rule with an $answer$ predicate in the head. The answer predicate contains at least two arguments; the first of these is the question number, and any remaining arguments store the answer to the question. For example, the following is the answer predicate for property questions:
$$answer(\textit{$<$question$>$}, \textit{$<$property$>$}, \textit{$<$value$>$})$$

After the ASP program has been run, the answer predicates are collated. If there are multiple answers for a single question, a single answer is chosen at random. After this, hardcoded rules generate an answer string using the arguments of the $answer$ predicate, for each question. These answer strings are considered the output of the H-PERL model, and are therefore compared against the expected answer string when evaluating a model's performance.

Before describing the ASP rule templates, we provide a definition of an ASP object descriptor. This discriptor is generated using information extracted from the \textit{$<$object$>$} part of the question. As a reminder, the \textit{$<$object$>$} grammar rule contains information on an object's class and, optionally, a property value. For example, `silver fish', `upward-facing rock' and `octopus' are all valid instances of the \textit{$<$object$>$} EBNF rule. The ASP object descriptor is a set of predicates used in the body of a rule which locate the object mentioned in the question. For object identifier \textit{$<$id$>$}, frame \textit{$<$frame$>$} and an instance of \textit{$<$object$>$} with class \textit{$<$class$>$} and property value \textit{$<$value$>$}, the ASP object descriptor is as follows:
\begin{equation}
  \label{eqn:asp_obj}
  obs(class(\textit{$<$class$>$}, \textit{$<$id$>$}), \textit{$<$frame$>$}), [ obs(\textit{$<$property$>$}(\textit{$<$value$>$}, \textit{$<$id$>$}), \textit{$<$frame$>$}) ]
\end{equation}

If \textit{$<$object$>$} does not contain a property value, the property predicate in~\ref{eqn:asp_obj} is omitted. In order to simplify the ASP rule templates, which follow, we use the following \textsc{AspObj} function, which constructs the set of predicates shown in~\ref{eqn:asp_obj} to be used in the body of ASP rules:
$$\textsc{AspObj}(\textit{$<$object$>$}, \textit{$<$id$>$}, \textit{$<$frame$>$})$$

We also outline a number of ASP helper rules whose head predicates are used in the body of the question-answering rules. For brevity the full helper rules are not shown, however, the descriptions of the head predicates are as follows:
\begin{enumerate}
  \item If $changed(\textit{$<$property$>$}, \textit{$<$before$>$}, \textit{$<$after$>$}, \textit{$<$id$>$}, \textit{$<$frame$>$})$ is true, then the property \textit{$<$property$>$} of the object with identifer \textit{$<$id$>$} takes value \textit{$<$before$>$} in frame \textit{$<$frame$>$} and value \textit{$<$after$>$} in frame \textit{$<$frame$>$}+1.

  \item The $event\_count(\textit{$<$event$>$}, \textit{$<$id$>$}, \textit{$<$num$>$})$ predicate means that \textit{$<$num$>$} is the number of times an object with identifer \textit{$<$id$>$} causes event \textit{$<$event$>$}.

  \item If $event\_occurrence(\textit{$<$event$>$}, \textit{$<$id$>$}, \textit{$<$frame$>$}, \textit{$<$num$>$})$ is true, then \textit{$<$num$>$} is the number of times an object with identifer \textit{$<$id$>$} causes event \textit{$<$event$>$} during or before frame \textit{$<$frame$>$}.

  \item The $exists(\textit{$<$id$>$}, \textit{$<$frame$>$})$ predicate means that the object with identifier \textit{$<$id$>$} exists in frame \textit{$<$frame$>$}.
\end{enumerate}

In the following, we show, firstly, the question template (as already outlined in Chapter~\ref{chapter:dataset}), secondly, how the ASP rule is constucted from the information in the parsed question and, finally, how the answer string is generated from the $answer$ predicate, for each type of question:
\begin{enumerate}
  \item Property questions have the following template:
  $$\text{What \textit{$<$property$>$} was the \textit{$<$object$>$} in frame \textit{$<$frame$>$}?}$$
  Following question parsing, the ASP rule for question number \textit{$<$question$>$} is constructed as follows:
  \begin{equation}
    \begin{split}
      answer(\textit{$<$question$>$}, \textit{$<$property$>$}, Value) \text{ :- } & \textsc{AspObj}(\textit{$<$object$>$}, Id, \textit{$<$frame$>$}), \\
      & obs(\textit{$<$property$>$}(Value, Id), \textit{$<$frame$>$}).
    \end{split}
  \end{equation}
  Then the answer string is simply the $Value$ argument to the $answer$ predicate.

  \item Relation questions have the following template:
  $$\text{Was the \textit{$<$object$>$} \textit{$<$relation$>$} to the \textit{$<$object$>$} in frame \textit{$<$frame$>$}?}$$
  The ASP rule for answer these questions is generated as follows:
  \begin{equation}
    \begin{split}
      related(\textit{$<$question$>$}) \text{ :- } & holds(\textit{$<$relation$>$}(Id1, Id2), \textit{$<$frame$>$}), \\
      & \textsc{AspObj}(\textit{$<$object$>$}, Id1, \textit{$<$frame$>$}), \\
      & \textsc{AspObj}(\textit{$<$object$>$}, Id2, \textit{$<$frame$>$}). \\
      answer(\textit{$<$question$>$}, yes) \text{ :- } & related(\textit{$<$question$>$}). \\
      answer(\textit{$<$question$>$}, no) \text{ :- } & not \text{ } related(\textit{$<$question$>$}). \\
    \end{split}
  \end{equation}
  The yes or no answer string is then taken from the $answer$ predicate.

  \item Action questions have the following form:
  $$\text{Which action occurred immediately after frame \textit{$<$frame$>$}?}$$
  We create an ASP rule for each action, including the `nothing' action. The outline of each rule is as follows:
  \begin{equation}
    answer(\textit{$<$question$>$}, \textit{$<$action$>$}) \text{ :- } occurs(\textit{$<$action$>$}(Id), \textit{$<$frame$>$}).
  \end{equation}
  The answer string is then simply the value of \textit{$<$action$>$}.

  \item Changed-property questions take the following form:
  $$\text{What happened to the octopus immediately after \textit{$<$frame$>$?}}$$
  The ASP rule for answering this question is as follows:
  \begin{equation}
    \begin{split}
      answer(\textit{$<$question$>$}, Prop, Before, After) \text{ :- } & changed(Prop, Before, After, Id, \textit{$<$frame$>$}), \\
      & exists(Id, \textit{$<$frame$>$}+1), \\
      & \textsc{AspObj}(\text{octopus}, Id, \textit{$<$frame$>$}).
    \end{split}
  \end{equation}
  The answer string is then constructed as follows:
  $$\text{Its $Prop$ changed from $Before$ to $After$}$$

\end{enumerate}



\end{document}
