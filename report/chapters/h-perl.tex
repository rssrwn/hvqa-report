\documentclass[../report.tex]{subfiles}


\begin{document}


\chapter{Hybrid Property, Event and Relation Learner}
\chaptermark{H-PERL}
\label{chapter:h-perl}

We outline a novel paradigm for solving the VideoQA problem. This structure merges deep learning and logic-based machine learning and inference methods in an attempt to learn the concepts required to answer the questions. We name this structure: Hybrid Property, Event and Relation Learner (H-PERL). Section~\ref{section:h-perl-arch} outlines the structure of an H-PERL model, while Section~\ref{section:common-components} discusses the implementation of a number of H-PERL components which are common to all models shown in this report.

\section{Architecture}
\label{section:h-perl-arch}

% Introduce Hybrid VideoQA Model architecture. Talk about how this model models an environment using objects, properties, relations, actions and events. (show generic architecture diagram)

H-PERL is a generic, pipelined structure for finding an answer to a set of questions, given a video. The pipeline is composed of a number of components which, when strung together, form a model (a model can be thought of as a specific `instance' of the H-PERL pipeline). Chapters~\ref{chapter:hardcoded} and~\ref{chapter:trained} each outline an H-PERL model for the OceanQA dataset.

An H-PERL pipeline assumes that all of the information in an environment which is required to answer the questions can be modeled using: objects; binary relations between objects; and events between two consecutive frames in the video. For many environments, simple environments (like our OceanQA dataset) in particular, this assumption holds. However for many other VideoQA environments, particularly those set in the real-world, this assumption may not be suitable. For example, we may not always be able to extract objects from a video (as in the case of abstract nouns), and yet information on these objects may still be required to answer the questions. Additionally, the H-PERL structure is not capable of modelling relations between objects with an arity larger than 2.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{h-perl-pipeline}
  \caption{An H-PERL pipeline for VideoQA. Green, red and blue shading indicates components which work to extract features at different levels of abstraction. Grey shading indicates the `core' components of the pipeline.}
  \label{fig:h-perl-pipeline}
\end{figure}

Figure~\ref{fig:h-perl-pipeline} shows the components involved in a typical H-PERL pipeline. During evaluation, information from the video and the question flow, from left to right, through the pipeline. Each H-PERL model assumes that the object detection and question parsing components are ``pre-made'' (either pre-trained or manually engineered), we refer to these as `non-core' components. H-PERL allows the remaining, `core' components to be updated as the model is trained, although they don't necessarily have to be. Each core component in the pipeline accumulates information. This means that each component guarantees to add more features to the data, rather than overwrite existing features (with the small exception of the event component when error correction is used, discussed further in Chapter~\ref{chapter:hardcoded}). As shown in Figure~\ref{fig:h-perl-pipeline} components in the pipeline work at different levels of abstraction; the object properties and tracking components work to extract object-level features, while the relations and events components work to extract frame and video-level features, respectively.

The following is a high-level description of the tasks each component is required to complete for the H-PERL pipeline to work with high accuracy:
\begin{enumerate}
  \item \textbf{Question Parser}. The QA parsing component is used to extract relevant pieces of information from the questions (and answers when training). For example, given the question: ``What does the octopus do immediately after eating a fish?", the parsing component would

  \item \textbf{Object Detector}. Produces bounding boxes and classes for each object in each frame of the video. The object detector also needs run an object tracking algorithm to assign each object in the video a unqiue identifier. Ideally the object tracker would correctly identify all objects in the video, however errors in the object detection stage can confuse the tracker as to which identifier should be assigned to each object. In this case some form of error correction may be helpful. TODO

  \item \textbf{Property Extractor}. Given a set of properties, classifies the value of each property for each object in each frame of the video. As mentioned in Chapter~\ref{chapter:dataset}, possible properties could be colour or rotation. The property component does not classify the position or class of an object (this is always handled by the detector).

  \item \textbf{Object Tracker}.

  \item \textbf{Relation Classifier}.

  \item \textbf{Event Detector}.

  \item \textbf{QA System}.
\end{enumerate}

% TODO add this to property extractor discussion
% However, since the \textbf{object} rule in the grammar may contain a property value, knowledge of object properties may be required to work out which object a question is referring to, creating a `chicken-and-egg' problem. For example, if a question asked ``What colour was the upward-facing fish in frame 12?", and there three fish (with unique rotations) in frame 12, one would need knowledge of object properties in order to select a image of a fish to train with. A model would need to be capable of overcoming this problem if it is to make full use of the dataset.

\section{Common Components}
\label{section:common-components}

Give an intro to the each of the components common to all models: object detector, tracker, (QA system), QA parser.



\end{document}
